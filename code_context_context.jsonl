{"timestamp": "2024-11-08T23:12:57.390885", "repository_root": "/home/anthony_z/ai/code_context", "total_files": 14, "statistics": {"processed_files": 14, "skipped_files": 34, "failed_files": 0, "total_raw_size": 53874, "total_cleaned_size": 46229, "cache_hits": 0, "processing_time": 0, "total_files": 48, "file_types": {"py": 14}}}
{"file_path": "exceptions.py", "file_type": "source", "analysis": {"success": true, "metrics": {"lines_of_code": 11, "comment_lines": 0, "blank_lines": 2, "complexity": [["CodeContextError", 1, 0, 3, [], [], 1], ["FileProcessingError", 5, 0, 7, [], [], 1], ["ConfigurationError", 9, 0, 11, [], [], 1]], "maintainability_index": 100.0, "max_depth": 0}, "classes": [{"name": "CodeContextError", "base_classes": ["Exception"]}, {"name": "FileProcessingError", "base_classes": ["CodeContextError"]}, {"name": "ConfigurationError", "base_classes": ["CodeContextError"]}]}, "size": 337, "content": "class CodeContextError(Exception):\n    \"\"\"Base exception for all code context related errors.\"\"\"\n    pass\n\nclass FileProcessingError(CodeContextError):\n    \"\"\"Raised when there's an error processing a specific file.\"\"\"\n    pass\n\nclass ConfigurationError(CodeContextError):\n    \"\"\"Raised when there's an invalid configuration.\"\"\"\n    pass", "content_hash": -2211541311860289641}
{"file_path": "cli.py", "file_type": "source", "analysis": {"success": true, "metrics": {"lines_of_code": 106, "comment_lines": 0, "blank_lines": 5, "complexity": [["format_size", 17, 0, 23, false, null, [], 3], ["parse_args", 25, 0, 46, false, null, [], 3], ["setup_logging", 48, 0, 51, false, null, [], 2], ["display_summary", 53, 0, 76, false, null, [], 5], ["main", 78, 0, 102, false, null, [], 7]], "maintainability_index": 37.94, "max_depth": 2}, "imports": ["pathlib", "processors", "asyncio", "datetime", "aiofiles", "typing", "sys", "rich", "argparse", "logging", "config"], "functions": [{"name": "format_size", "arguments": ["size_in_bytes"], "decorators": [], "is_async": false}, {"name": "parse_args", "arguments": [], "decorators": [], "is_async": false}, {"name": "setup_logging", "arguments": ["verbose"], "decorators": [], "is_async": false}, {"name": "display_summary", "arguments": ["stats"], "decorators": [], "is_async": false}]}, "size": 5609, "content": "import asyncio\nimport argparse\nimport logging\nimport sys\nfrom pathlib import Path\nfrom typing import Optional\nfrom rich.console import Console\nfrom rich.table import Table\nfrom rich import print as rprint\nfrom rich.panel import Panel\nfrom config import ProcessorConfig\nfrom processors.optimized_processor import OptimizedContentProcessor\nimport aiofiles\nfrom datetime import datetime\nconsole = Console()\n\ndef format_size(size_in_bytes: int) -> str:\n    \"\"\"Format size in bytes to human readable format.\"\"\"\n    for unit in ['B', 'KB', 'MB', 'GB']:\n        if size_in_bytes < 1024:\n            return f'{size_in_bytes:.2f} {unit}'\n        size_in_bytes /= 1024\n    return f'{size_in_bytes:.2f} TB'\n\ndef parse_args() -> ProcessorConfig:\n    \"\"\"Parse command line arguments and return ProcessorConfig.\"\"\"\n    parser = argparse.ArgumentParser(description='Code Context Generator - Analyze and process code repositories', formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n    parser.add_argument('path', type=str, help='Path to the code directory to analyze')\n    parser.add_argument('-o', '--output', type=str, help='Output file path (default: <directory_name>_context.<format>)')\n    parser.add_argument('-f', '--format', choices=['jsonl', 'json'], default='jsonl', help='Output format')\n    parser.add_argument('--include-tests', action='store_true', help='Include test files in analysis')\n    parser.add_argument('--max-size', type=int, default=1024 * 1024, help='Maximum file size to process in bytes')\n    parser.add_argument('--workers', type=int, default=4, help='Number of worker processes')\n    parser.add_argument('--no-cache', action='store_true', help='Disable caching of analysis results')\n    parser.add_argument('-v', '--verbose', action='store_true', help='Enable verbose logging')\n    parser.add_argument('--optimize', action='store_true', help='Use optimized output format', default=True)\n    args = parser.parse_args()\n    path = Path(args.path)\n    if not path.exists():\n        console.print(f'[red]Error:[/red] Path does not exist: {path}')\n        sys.exit(1)\n    if not path.is_dir():\n        console.print(f'[red]Error:[/red] Path is not a directory: {path}')\n        sys.exit(1)\n    config = ProcessorConfig(target_dir=path, output_file=args.output, output_format=args.format, include_tests=args.include_tests, max_file_size=args.max_size, worker_count=args.workers, cache_enabled=not args.no_cache, verbose=args.verbose, optimize=args.optimize)\n    return config\n\ndef setup_logging(verbose: bool):\n    \"\"\"Setup logging configuration\"\"\"\n    log_level = logging.DEBUG if verbose else logging.INFO\n    logging.basicConfig(level=log_level, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s', handlers=[logging.StreamHandler(sys.stdout)])\n\ndef display_summary(stats: dict):\n    \"\"\"Display a formatted summary of the processing results.\"\"\"\n    table = Table(title='Processing Summary', show_header=True, header_style='bold blue')\n    table.add_column('Metric', style='cyan')\n    table.add_column('Value', justify='right', style='green')\n    table.add_row('Files Processed', str(stats['processed_files']))\n    table.add_row('Files Skipped', str(stats['skipped_files']))\n    table.add_row('Files Failed', str(stats['failed_files']))\n    table.add_row('Cache Hits', str(stats['cache_hits']))\n    table.add_row('Total Files', str(stats['total_files']))\n    table.add_row('Original Size', format_size(stats['total_raw_size']))\n    table.add_row('Cleaned Size', format_size(stats['total_cleaned_size']))\n    if stats['total_raw_size'] > 0:\n        reduction = (1 - stats['total_cleaned_size'] / stats['total_raw_size']) * 100\n        table.add_row('Size Reduction', f'{reduction:.2f}%')\n    table.add_row('Processing Time', f\"{stats['processing_time']:.2f} seconds\")\n    if stats['processed_files'] > 0:\n        avg_time = stats['processing_time'] / stats['processed_files']\n        table.add_row('Average Time per File', f'{avg_time:.3f} seconds')\n    if stats.get('file_types'):\n        table.add_row('File Types', ', '.join((f'{k}: {v}' for (k, v) in stats['file_types'].items())))\n    console.print('\\n')\n    console.print(table)\n    console.print('\\n')\n\nasync def main():\n    try:\n        console.print(Panel.fit('[bold blue]Code Context Generator[/bold blue]\\n[dim]Analyzing your code repository...[/dim]', border_style='blue'))\n        config = parse_args()\n        setup_logging(config.verbose)\n        processor = OptimizedContentProcessor(config)\n        with console.status('[bold green]Processing files...') as status:\n            stats = await processor.process()\n        display_summary(stats)\n        if config.output_file:\n            console.print(f'Results written to: [bold]{config.output_file}[/bold]')\n        if config.verbose:\n            console.print('\\n[bold]Additional Information:[/bold]')\n            if hasattr(processor, 'cache'):\n                console.print(f'\u2022 Cache directory: {processor.cache.cache_dir}')\n            console.print(f\"\u2022 Excluded patterns: {', '.join(config.excluded_patterns)}\")\n            console.print(f\"\u2022 Included extensions: {', '.join(config.included_extensions)}\")\n    except KeyboardInterrupt:\n        console.print('\\n[yellow]Process interrupted by user[/yellow]')\n        sys.exit(1)\n    except Exception as e:\n        console.print(f'\\n[red]Error:[/red] {str(e)}')\n        if config.verbose:\n            console.print_exception()\n        sys.exit(1)\nif __name__ == '__main__':\n    if sys.platform == 'win32':\n        asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())\n    asyncio.run(main())", "content_hash": 963905162087882222}
{"file_path": "config.py", "file_type": "source", "analysis": {"success": true, "metrics": {"lines_of_code": 24, "comment_lines": 0, "blank_lines": 2, "complexity": [["ProcessorConfig", 6, 0, 24, [["__post_init__", 19, 4, 24, true, "ProcessorConfig", [], 3]], [], 4], ["__post_init__", 19, 4, 24, true, "ProcessorConfig", [], 3]], "maintainability_index": 56.77, "max_depth": 1}, "imports": ["pathlib", "dataclasses", "typing"], "functions": [{"name": "__post_init__", "arguments": ["self"], "decorators": [], "is_async": false}], "classes": [{"name": "ProcessorConfig", "methods": [{"name": "__post_init__", "is_private": true, "is_async": false}], "decorators": ["dataclass"]}]}, "size": 908, "content": "from dataclasses import dataclass\nfrom typing import Set, Dict, Optional, List\nfrom pathlib import Path\n\n@dataclass\nclass ProcessorConfig:\n    target_dir: Path\n    output_file: Optional[str] = None\n    include_tests: bool = False\n    output_format: str = 'jsonl'\n    max_file_size: int = 1024 * 1024\n    worker_count: int = 4\n    cache_enabled: bool = True\n    verbose: bool = False\n    optimize: bool = True\n    excluded_patterns: Set[str] = None\n    included_extensions: Set[str] = None\n\n    def __post_init__(self):\n        self.target_dir = Path(self.target_dir)\n        if self.excluded_patterns is None:\n            self.excluded_patterns = {'node_modules', '.git', 'venv', '__pycache__', 'dist', 'build', '.env', '.pytest_cache'}\n        if self.included_extensions is None:\n            self.included_extensions = {'.py', '.js', '.jsx', '.ts', '.tsx', '.java', '.cpp', '.c', '.h', '.cs', '.go', '.rb'}", "content_hash": -997023983724803827}
{"file_path": "__init__.py", "file_type": "source", "analysis": {"success": true, "metrics": {"lines_of_code": 3, "comment_lines": 0, "blank_lines": 0, "complexity": [], "maintainability_index": 100.0, "max_depth": 0}, "imports": ["main"]}, "size": 87, "content": "from .main import generate_context\n__version__ = '1.0.0'\n__all__ = ['generate_context']", "content_hash": -3603465451575230674}
{"file_path": "main.py", "file_type": "source", "analysis": {"success": true, "metrics": {"lines_of_code": 35, "comment_lines": 0, "blank_lines": 2, "complexity": [["generate_context", 12, 0, 28, false, null, [], 4], ["main", 30, 0, 33, false, null, [], 1]], "maintainability_index": 58.75, "max_depth": 1}, "imports": ["pathlib", "processors", "asyncio", "code_context", "sys", "utils", "os", "argparse", "logging"], "functions": [{"name": "generate_context", "arguments": ["directory", "output_file", "include_tests", "format", "verbose"], "decorators": [], "is_async": false}]}, "size": 1415, "content": "import asyncio\nimport os\nimport sys\nimport argparse\nimport logging\nfrom pathlib import Path\nfrom code_context.config import ProcessorConfig\nfrom code_context.processors.async_processor import AsyncContentProcessor\nfrom processors.content_processor import ContentProcessor\nfrom utils.file_utils import setup_logging\n\ndef generate_context(directory: str, output_file: str=None, include_tests: bool=False, format: str='jsonl', verbose: bool=False) -> str:\n    setup_logging(verbose)\n    logger = logging.getLogger(__name__)\n    if not output_file:\n        output_file = f'{Path(directory).name}_context.{format}'\n    try:\n        processor = ContentProcessor(target_dir=directory, include_tests=include_tests, output_format=format)\n        processor.process(output_file)\n        logger.info(f'\\nGenerated context file: {output_file}')\n        if include_tests:\n            logger.info('Test files were included in the generated context')\n        else:\n            logger.info('Test files were excluded from the generated context')\n        return output_file\n    except Exception as e:\n        logger.error(f'Error: {e}')\n        raise\n\nasync def main():\n    config = ProcessorConfig(target_dir='path/to/your/code', include_tests=True, output_format='jsonl', cache_enabled=True, verbose=True)\n    processor = AsyncContentProcessor(config)\n    await processor.process()\nif __name__ == '__main__':\n    asyncio.run(main())", "content_hash": -2990149260903627927}
{"file_path": "utils/file_utils.py", "file_type": "source", "analysis": {"success": true, "metrics": {"lines_of_code": 45, "comment_lines": 0, "blank_lines": 3, "complexity": [["get_file_encoding", 7, 0, 12, false, null, [], 2], ["read_file_safely", 14, 0, 37, false, null, [], 5], ["calculate_file_hash", 39, 0, 45, false, null, [], 2]], "maintainability_index": 58.91, "max_depth": 2}, "imports": ["pathlib", "typing", "hashlib", "chardet", "logging"], "functions": [{"name": "get_file_encoding", "arguments": ["file_path"], "decorators": [], "is_async": false}, {"name": "read_file_safely", "arguments": ["file_path", "logger"], "decorators": [], "is_async": false}, {"name": "calculate_file_hash", "arguments": ["file_path"], "decorators": [], "is_async": false}]}, "size": 1770, "content": "import hashlib\nimport logging\nfrom pathlib import Path\nfrom typing import Optional, Dict\nimport chardet\n\ndef get_file_encoding(file_path: Path) -> str:\n    \"\"\"Detect file encoding using chardet.\"\"\"\n    with open(file_path, 'rb') as f:\n        raw_data = f.read()\n        result = chardet.detect(raw_data)\n        return result['encoding'] or 'utf-8'\n\ndef read_file_safely(file_path: Path, logger: logging.Logger) -> Optional[str]:\n    \"\"\"Safely read a file with multiple encoding fallbacks.\"\"\"\n    try:\n        encoding = get_file_encoding(file_path)\n        with open(file_path, 'r', encoding=encoding) as f:\n            return f.read()\n    except UnicodeDecodeError:\n        logger.warning(f'Failed to read {file_path} with detected encoding {encoding}')\n        for enc in ['utf-8', 'latin-1', 'cp1252']:\n            try:\n                with open(file_path, 'r', encoding=enc) as f:\n                    content = f.read()\n                logger.info(f'Successfully read {file_path} with {enc} encoding')\n                return content\n            except UnicodeDecodeError:\n                continue\n        try:\n            with open(file_path, 'r', encoding='utf-8', errors='replace') as f:\n                content = f.read()\n            logger.warning(f'Read {file_path} with replacement characters')\n            return content\n        except Exception as e:\n            logger.error(f'Failed to read {file_path} with any encoding: {e}')\n            return None\n\ndef calculate_file_hash(file_path: Path) -> str:\n    \"\"\"Calculate SHA256 hash of a file.\"\"\"\n    sha256_hash = hashlib.sha256()\n    with open(file_path, 'rb') as f:\n        for byte_block in iter(lambda : f.read(4096), b''):\n            sha256_hash.update(byte_block)\n    return sha256_hash.hexdigest()", "content_hash": -2368180073366798753}
{"file_path": "utils/cache_utils.py", "file_type": "source", "analysis": {"success": true, "metrics": {"lines_of_code": 40, "comment_lines": 0, "blank_lines": 5, "complexity": [["FileAnalysisCache", 6, 0, 40, [["__init__", 8, 4, 12, true, "FileAnalysisCache", [], 1], ["get_cache_path", 14, 4, 15, true, "FileAnalysisCache", [], 1], ["get", 17, 4, 31, true, "FileAnalysisCache", [], 4], ["set", 33, 4, 40, true, "FileAnalysisCache", [], 2]], [], 9], ["__init__", 8, 4, 12, true, "FileAnalysisCache", [], 1], ["get_cache_path", 14, 4, 15, true, "FileAnalysisCache", [], 1], ["get", 17, 4, 31, true, "FileAnalysisCache", [], 4], ["set", 33, 4, 40, true, "FileAnalysisCache", [], 2]], "maintainability_index": 56.5, "max_depth": 2}, "imports": ["pathlib", "logging", "pickle", "typing"], "functions": [{"name": "__init__", "arguments": ["self", "cache_dir"], "decorators": [], "is_async": false}, {"name": "get_cache_path", "arguments": ["self", "file_hash"], "decorators": [], "is_async": false}, {"name": "get", "arguments": ["self", "file_hash"], "decorators": [], "is_async": false}, {"name": "set", "arguments": ["self", "file_hash", "data"], "decorators": [], "is_async": false}], "classes": [{"name": "FileAnalysisCache", "methods": [{"name": "__init__", "is_private": true, "is_async": false}, {"name": "get_cache_path", "is_private": false, "is_async": false}, {"name": "get", "is_private": false, "is_async": false}, {"name": "set", "is_private": false, "is_async": false}]}]}, "size": 1541, "content": "import pickle\nfrom pathlib import Path\nfrom typing import Dict, Any, Optional\nimport logging\n\nclass FileAnalysisCache:\n\n    def __init__(self, cache_dir: Path):\n        self.cache_dir = cache_dir\n        self.cache_dir.mkdir(parents=True, exist_ok=True)\n        self.memory_cache: Dict[str, Any] = {}\n        self.logger = logging.getLogger('code_context.cache')\n\n    def get_cache_path(self, file_hash: str) -> Path:\n        return self.cache_dir / f'{file_hash}.pickle'\n\n    def get(self, file_hash: str) -> Optional[Dict[str, Any]]:\n        if file_hash in self.memory_cache:\n            self.logger.debug(f'Cache hit (memory) for {file_hash}')\n            return self.memory_cache[file_hash]\n        cache_path = self.get_cache_path(file_hash)\n        if cache_path.exists():\n            try:\n                with open(cache_path, 'rb') as f:\n                    result = pickle.load(f)\n                self.memory_cache[file_hash] = result\n                self.logger.debug(f'Cache hit (disk) for {file_hash}')\n                return result\n            except Exception as e:\n                self.logger.warning(f'Failed to load cache for {file_hash}: {e}')\n        return None\n\n    def set(self, file_hash: str, data: Dict[str, Any]):\n        self.memory_cache[file_hash] = data\n        cache_path = self.get_cache_path(file_hash)\n        try:\n            with open(cache_path, 'wb') as f:\n                pickle.dump(data, f)\n        except Exception as e:\n            self.logger.warning(f'Failed to save cache for {file_hash}: {e}')", "content_hash": 4078054059609417040}
{"file_path": "utils/logging_utils.py", "file_type": "source", "analysis": {"success": true, "metrics": {"lines_of_code": 24, "comment_lines": 0, "blank_lines": 1, "complexity": [["setup_advanced_logging", 7, 0, 24, false, null, [], 3]], "maintainability_index": 65.15, "max_depth": 0}, "imports": ["pathlib", "datetime", "code_context", "sys", "logging"], "functions": [{"name": "setup_advanced_logging", "arguments": ["config"], "decorators": [], "is_async": false}]}, "size": 1189, "content": "import logging\nimport sys\nfrom pathlib import Path\nfrom datetime import datetime\nfrom code_context.config import ProcessorConfig\n\ndef setup_advanced_logging(config: ProcessorConfig) -> logging.Logger:\n    \"\"\"Sets up advanced logging with file and console output.\"\"\"\n    logger = logging.getLogger('code_context')\n    logger.setLevel(logging.DEBUG if config.verbose else logging.INFO)\n    logger.handlers = []\n    console_handler = logging.StreamHandler(sys.stdout)\n    console_handler.setLevel(logging.DEBUG if config.verbose else logging.INFO)\n    console_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s', datefmt='%Y-%m-%d %H:%M:%S')\n    console_handler.setFormatter(console_formatter)\n    logger.addHandler(console_handler)\n    log_dir = Path('logs')\n    log_dir.mkdir(exist_ok=True)\n    file_handler = logging.FileHandler(log_dir / f\"code_context_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log\")\n    file_handler.setLevel(logging.DEBUG)\n    file_formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s', datefmt='%Y-%m-%d %H:%M:%S')\n    file_handler.setFormatter(file_formatter)\n    logger.addHandler(file_handler)\n    return logger", "content_hash": -9099176619947691438}
{"file_path": "parsers/gitignore_parser.py", "file_type": "source", "analysis": {"success": true, "metrics": {"lines_of_code": 63, "comment_lines": 0, "blank_lines": 7, "complexity": [["GitignorePattern", 6, 0, 33, [["__init__", 8, 4, 14, true, "GitignorePattern", [], 2], ["_glob_to_regex", 16, 4, 30, true, "GitignorePattern", [], 7], ["matches", 32, 4, 33, true, "GitignorePattern", [], 1]], [], 11], ["__init__", 8, 4, 14, true, "GitignorePattern", [], 2], ["_glob_to_regex", 16, 4, 30, true, "GitignorePattern", [], 7], ["matches", 32, 4, 33, true, "GitignorePattern", [], 1], ["GitignoreParser", 35, 0, 63, [["__init__", 37, 4, 51, true, "GitignoreParser", [], 6], ["is_ignored", 53, 4, 63, true, "GitignoreParser", [], 6]], [], 13], ["__init__", 37, 4, 51, true, "GitignoreParser", [], 6], ["is_ignored", 53, 4, 63, true, "GitignoreParser", [], 6]], "maintainability_index": 43.75, "max_depth": 4}, "imports": ["pathlib", "re", "logging"], "functions": [{"name": "__init__", "arguments": ["self", "pattern"], "decorators": [], "is_async": false}, {"name": "_glob_to_regex", "arguments": ["self", "pattern"], "decorators": [], "is_async": false}, {"name": "matches", "arguments": ["self", "path"], "decorators": [], "is_async": false}, {"name": "__init__", "arguments": ["self", "gitignore_path"], "decorators": [], "is_async": false}, {"name": "is_ignored", "arguments": ["self", "path"], "decorators": [], "is_async": false}], "classes": [{"name": "GitignorePattern", "methods": [{"name": "__init__", "is_private": true, "is_async": false}, {"name": "_glob_to_regex", "is_private": true, "is_async": false}, {"name": "matches", "is_private": false, "is_async": false}]}, {"name": "GitignoreParser", "methods": [{"name": "__init__", "is_private": true, "is_async": false}, {"name": "is_ignored", "is_private": false, "is_async": false}]}]}, "size": 2099, "content": "import re\nimport logging\nfrom pathlib import Path\nlogger = logging.getLogger(__name__)\n\nclass GitignorePattern:\n\n    def __init__(self, pattern: str):\n        self.raw_pattern = pattern\n        self.is_negated = pattern.startswith('!')\n        self.is_directory_only = pattern.endswith('/')\n        pattern = pattern[1:] if self.is_negated else pattern\n        pattern = pattern.rstrip('/')\n        self.regex = self._glob_to_regex(pattern)\n\n    def _glob_to_regex(self, pattern: str) -> re.Pattern:\n        regex = ''\n        if not pattern.startswith('/'):\n            regex = '(?:.+/)?'\n        for c in pattern:\n            if c == '*':\n                regex += '[^/]*'\n            elif c == '?':\n                regex += '[^/]'\n            elif c in '.[]()+^${}|':\n                regex += f'\\\\{c}'\n            else:\n                regex += c\n        regex = f'^{regex}(?:/.*)?$' if self.is_directory_only else f'^{regex}$'\n        return re.compile(regex)\n\n    def matches(self, path: str) -> bool:\n        return bool(self.regex.match(path))\n\nclass GitignoreParser:\n\n    def __init__(self, gitignore_path: Path):\n        self.patterns = []\n        self.negated_patterns = []\n        if not gitignore_path.exists():\n            logger.warning(f'No .gitignore found at {gitignore_path}')\n            return\n        with open(gitignore_path, 'r') as f:\n            for line in f:\n                line = line.strip()\n                if line and (not line.startswith('#')):\n                    pattern = GitignorePattern(line)\n                    if pattern.is_negated:\n                        self.negated_patterns.append(pattern)\n                    else:\n                        self.patterns.append(pattern)\n\n    def is_ignored(self, path: str) -> bool:\n        path = path.replace('\\\\', '/')\n        if path.startswith('./'):\n            path = path[2:]\n        for pattern in self.negated_patterns:\n            if pattern.matches(path):\n                return False\n        for pattern in self.patterns:\n            if pattern.matches(path):\n                return True\n        return False", "content_hash": -5831614958494013621}
{"file_path": "parsers/file_parser.py", "file_type": "source", "analysis": {"success": true, "metrics": {"lines_of_code": 46, "comment_lines": 0, "blank_lines": 5, "complexity": [["FileParser", 7, 0, 46, [["__init__", 9, 4, 13, true, "FileParser", [], 1], ["is_test_file", 15, 4, 17, true, "FileParser", [], 2], ["should_process_file", 19, 4, 38, true, "FileParser", [], 13], ["get_file_type", 40, 4, 46, true, "FileParser", [], 3]], [], 20], ["__init__", 9, 4, 13, true, "FileParser", [], 1], ["is_test_file", 15, 4, 17, true, "FileParser", [], 2], ["should_process_file", 19, 4, 38, true, "FileParser", [], 13], ["get_file_type", 40, 4, 46, true, "FileParser", [], 3]], "maintainability_index": 47.79, "max_depth": 2}, "imports": ["parsers", "pathlib", "logging", "fnmatch"], "functions": [{"name": "__init__", "arguments": ["self", "include_tests"], "decorators": [], "is_async": false}, {"name": "is_test_file", "arguments": ["self", "file_path"], "decorators": [], "is_async": false}, {"name": "should_process_file", "arguments": ["self", "file_path", "gitignore_parser"], "decorators": [], "is_async": false}, {"name": "get_file_type", "arguments": ["self", "file_path"], "decorators": [], "is_async": false}], "classes": [{"name": "FileParser", "methods": [{"name": "__init__", "is_private": true, "is_async": false}, {"name": "is_test_file", "is_private": false, "is_async": false}, {"name": "should_process_file", "is_private": false, "is_async": false}, {"name": "get_file_type", "is_private": false, "is_async": false}]}]}, "size": 2605, "content": "from pathlib import Path\nfrom fnmatch import fnmatch\nimport logging\nfrom parsers.gitignore_parser import GitignoreParser\nlogger = logging.getLogger(__name__)\n\nclass FileParser:\n\n    def __init__(self, include_tests: bool=False):\n        self.include_tests = include_tests\n        self.test_patterns = {'**/test_*.py', '**/tests/*.py', '**/tests/**/*.py', '**/*_test.py', '**/test/*.py', '**/test/**/*.py', '**/*.spec.js', '**/*.test.js', '**/test/*.js', '**/tests/*.js', '**/__tests__/**'}\n        self.ignore_patterns = {'node_modules', '.git', 'venv', '__pycache__', 'dist', 'build', '.min.', 'vendor/', '.DS_Store', '.env', '.coverage', 'bundle.', 'package-lock.json', 'README.md', '.pytest_cache', 'htmlcov', '.coverage', 'test-results', '*.pyc', '*.pyo'}\n        self.file_categories = {'source': {'.py', '.js', '.jsx', '.ts', '.tsx', '.java', '.cpp', '.c', '.h', '.cs', '.go', '.rb'}, 'config': {'.json', '.yaml', '.yml', '.toml', '.ini', '.xml'}, 'doc': {'.md', '.rst', '.txt'}, 'style': {'.css', '.scss', '.sass', '.less'}, 'template': {'.html', '.jinja', '.jinja2', '.tmpl'}, 'shell': {'.sh', '.bash', '.zsh', '.fish'}}\n\n    def is_test_file(self, file_path: str) -> bool:\n        normalized_path = str(file_path).replace('\\\\', '/')\n        return any((fnmatch(normalized_path, pattern) for pattern in self.test_patterns))\n\n    def should_process_file(self, file_path: Path, gitignore_parser) -> bool:\n        try:\n            if not file_path.exists() or not file_path.is_file():\n                return False\n            rel_path = str(file_path)\n            rel_path_str = rel_path.replace('\\\\', '/')\n            if gitignore_parser and gitignore_parser.is_ignored(rel_path_str):\n                return False\n            if any((p in rel_path for p in self.ignore_patterns)):\n                return False\n            is_test = self.is_test_file(rel_path_str)\n            if is_test and (not self.include_tests):\n                return False\n            valid_extensions = {ext for exts in self.file_categories.values() for ext in exts}\n            if file_path.suffix.lower() not in valid_extensions:\n                return False\n            return True\n        except Exception as e:\n            logger.error(f'Error checking file {file_path}: {e}')\n            return False\n\n    def get_file_type(self, file_path: Path) -> str:\n        \"\"\"Get the category of the file based on its extension.\"\"\"\n        ext = file_path.suffix.lower()\n        for (category, extensions) in self.file_categories.items():\n            if ext in extensions:\n                return category\n        return 'unknown'", "content_hash": 3116650053354464320}
{"file_path": "processors/content_processor.py", "file_type": "source", "analysis": {"success": true, "metrics": {"lines_of_code": 138, "comment_lines": 0, "blank_lines": 7, "complexity": [["ContentProcessor", 11, 0, 138, [["format_size", 14, 4, 20, true, "ContentProcessor", [], 3], ["__init__", 22, 4, 33, true, "ContentProcessor", [], 1], ["should_process_file", 35, 4, 65, true, "ContentProcessor", [], 15], ["process_file", 67, 4, 104, true, "ContentProcessor", [], 7], ["process_directory", 106, 4, 115, true, "ContentProcessor", [], 5], ["process", 117, 4, 138, true, "ContentProcessor", [], 5]], [], 37], ["format_size", 14, 4, 20, true, "ContentProcessor", [], 3], ["__init__", 22, 4, 33, true, "ContentProcessor", [], 1], ["should_process_file", 35, 4, 65, true, "ContentProcessor", [], 15], ["process_file", 67, 4, 104, true, "ContentProcessor", [], 7], ["process_directory", 106, 4, 115, true, "ContentProcessor", [], 5], ["process", 117, 4, 138, true, "ContentProcessor", [], 5]], "maintainability_index": 30.16, "max_depth": 2}, "imports": ["pathlib", "processors", "datetime", "json", "os", "parsers", "logging", "config"], "functions": [{"name": "format_size", "arguments": ["size_in_bytes"], "decorators": ["staticmethod"], "is_async": false}, {"name": "__init__", "arguments": ["self", "config"], "decorators": [], "is_async": false}], "classes": [{"name": "ContentProcessor", "methods": [{"name": "format_size", "is_private": false, "is_async": false}, {"name": "__init__", "is_private": true, "is_async": false}]}]}, "size": 7012, "content": "import json\nimport os\nfrom datetime import datetime\nfrom pathlib import Path\nimport logging\nfrom parsers.gitignore_parser import GitignoreParser\nfrom parsers.file_parser import FileParser\nfrom processors.code_analyzer import CodeAnalyzer\nfrom config import ProcessorConfig\n\nclass ContentProcessor:\n\n    @staticmethod\n    def format_size(size_in_bytes: int) -> str:\n        \"\"\"Format size in bytes to human readable format.\"\"\"\n        for unit in ['B', 'KB', 'MB', 'GB']:\n            if size_in_bytes < 1024:\n                return f'{size_in_bytes:.2f} {unit}'\n            size_in_bytes /= 1024\n        return f'{size_in_bytes:.2f} TB'\n\n    def __init__(self, config: ProcessorConfig):\n        self.target_dir = config.target_dir\n        self.gitignore = GitignoreParser(self.target_dir / '.gitignore')\n        self.file_parser = FileParser(config.include_tests)\n        self.code_analyzer = CodeAnalyzer()\n        self.output_format = config.output_format\n        self.config = config\n        self.seen_contents = set()\n        self.logger = logging.getLogger('code_context.processor')\n        self.stats = {'processed_files': 0, 'skipped_files': 0, 'failed_files': 0, 'total_raw_size': 0, 'total_cleaned_size': 0, 'cache_hits': 0, 'processing_time': 0, 'total_files': 0, 'file_types': {}}\n        self.ignored_prefixes = {'.git', '.cache', '__pycache__', 'node_modules', 'venv', 'dist', 'build'}\n        self.ignored_extensions = {'.pyc', '.pyo', '.pyd', '.so', '.dll', '.dylib', '.pickle', '.pkl', '.log', '.cache', '.jsonl'}\n\n    async def should_process_file(self, file_path: Path) -> bool:\n        \"\"\"Determine if a file should be processed with improved filtering.\"\"\"\n        try:\n            if not file_path.is_file():\n                return False\n            parts = file_path.parts\n            if any((part.startswith('.') for part in parts)):\n                return False\n            if any((part in self.ignored_prefixes for part in parts)):\n                return False\n            if file_path.suffix.lower() in self.ignored_extensions:\n                return False\n            if file_path.suffix.lower() not in self.config.included_extensions:\n                if self.config.verbose:\n                    rel_path = file_path.relative_to(self.config.target_dir)\n                    self.logger.debug(f'Ignoring {rel_path}: unsupported extension {file_path.suffix}')\n                return False\n            file_size = os.path.getsize(file_path)\n            if file_size > self.config.max_file_size:\n                if self.config.verbose:\n                    self.logger.debug(f'Ignoring {file_path.name}: exceeds size limit')\n                return False\n            rel_path = str(file_path.relative_to(self.config.target_dir))\n            if any((pattern in rel_path for pattern in self.config.excluded_patterns)):\n                if self.config.verbose:\n                    self.logger.debug(f'Ignoring {rel_path}: matches excluded pattern')\n                return False\n            return True\n        except Exception as e:\n            self.logger.warning(f'Error checking file {file_path}: {e}')\n            return False\n\n    async def process_file(self, file_path: Path):\n        \"\"\"Process a single file with improved handling.\"\"\"\n        try:\n            if not await self.should_process_file(file_path):\n                self.stats['skipped_files'] += 1\n                return None\n            raw_size = os.path.getsize(file_path)\n            self.stats['total_raw_size'] += raw_size\n            try:\n                with open(file_path, 'r', encoding='utf-8') as f:\n                    content = f.read()\n            except UnicodeDecodeError:\n                self.logger.warning(f'Failed to read {file_path} with UTF-8 encoding')\n                self.stats['failed_files'] += 1\n                return None\n            if not content.strip():\n                self.stats['skipped_files'] += 1\n                return None\n            file_type = self.file_parser.get_file_type(file_path)\n            cleaned_content = self.code_analyzer.clean_content(content, file_path.suffix.lstrip('.'))\n            cleaned_size = len(cleaned_content.encode('utf-8'))\n            self.stats['total_cleaned_size'] += cleaned_size\n            extension = file_path.suffix.lstrip('.')\n            self.stats['file_types'][extension] = self.stats['file_types'].get(extension, 0) + 1\n            content_hash = hash(cleaned_content)\n            if content_hash in self.seen_contents:\n                self.stats['skipped_files'] += 1\n                return None\n            self.seen_contents.add(content_hash)\n            analysis = self.code_analyzer.analyze_code(cleaned_content, file_path.suffix.lstrip('.'))\n            self.stats['processed_files'] += 1\n            if self.config.verbose:\n                self.logger.info(f'Processed {file_path.name} ({self.format_size(raw_size)} -> {self.format_size(cleaned_size)})')\n            return {'path': str(file_path.relative_to(self.config.target_dir)), 'type': file_type, 'analysis': analysis, 'size': cleaned_size, 'content': cleaned_content, 'hash': content_hash}\n        except Exception as e:\n            self.logger.error(f'Error processing {file_path}: {e}')\n            self.stats['failed_files'] += 1\n            return None\n\n    async def process_directory(self):\n        \"\"\"Process all files in the directory.\"\"\"\n        files = [f for f in self.target_dir.rglob('*') if f.is_file()]\n        self.stats['total_files'] = len(files)\n        results = []\n        for file_path in files:\n            result = await self.process_file(file_path)\n            if result:\n                results.append(result)\n        return results\n\n    async def process(self) -> dict:\n        start_time = datetime.now()\n        try:\n            self.logger.info(f'Starting processing of {self.target_dir}')\n            results = await self.process_directory()\n            output_file = self.config.output_file or f'{self.target_dir.name}_context.{self.output_format}'\n            if self.output_format == 'jsonl':\n                async with aiofiles.open(output_file, 'w') as f:\n                    metadata = {'timestamp': datetime.now().isoformat(), 'root': str(self.target_dir), 'file_count': len(results), 'stats': self.stats}\n                    await f.write(json.dumps(metadata) + '\\n')\n                    for result in results:\n                        await f.write(json.dumps(result) + '\\n')\n            else:\n                output = {'metadata': {'timestamp': datetime.now().isoformat(), 'root': str(self.target_dir), 'stats': self.stats}, 'files': results}\n                async with aiofiles.open(output_file, 'w') as f:\n                    await f.write(json.dumps(output, indent=2))\n            end_time = datetime.now()\n            self.stats['processing_time'] = (end_time - start_time).total_seconds()\n            return self.stats\n        except Exception as e:\n            self.logger.error(f'Error during processing: {e}')\n            raise", "content_hash": 3007185682547818214}
{"file_path": "processors/optimized_processor.py", "file_type": "source", "analysis": {"success": true, "metrics": {"lines_of_code": 123, "comment_lines": 0, "blank_lines": 12, "complexity": [["DescriptiveKeys", 15, 0, 40, [], [], 1], ["ContentOptimizer", 42, 0, 78, [["__init__", 45, 4, 46, true, "ContentOptimizer", [], 1], ["optimize_metrics", 48, 4, 50, true, "ContentOptimizer", [], 3], ["optimize_function", 52, 4, 54, true, "ContentOptimizer", [], 3], ["optimize_class", 56, 4, 59, true, "ContentOptimizer", [], 8], ["optimize_analysis", 61, 4, 74, true, "ContentOptimizer", [], 10], ["optimize_file_entry", 76, 4, 78, true, "ContentOptimizer", [], 3]], [], 29], ["__init__", 45, 4, 46, true, "ContentOptimizer", [], 1], ["optimize_metrics", 48, 4, 50, true, "ContentOptimizer", [], 3], ["optimize_function", 52, 4, 54, true, "ContentOptimizer", [], 3], ["optimize_class", 56, 4, 59, true, "ContentOptimizer", [], 8], ["optimize_analysis", 61, 4, 74, true, "ContentOptimizer", [], 10], ["optimize_file_entry", 76, 4, 78, true, "ContentOptimizer", [], 3], ["OptimizedContentProcessor", 80, 0, 123, [["__init__", 83, 4, 85, true, "OptimizedContentProcessor", [], 1], ["process_file", 87, 4, 92, true, "OptimizedContentProcessor", [], 2], ["process", 94, 4, 123, true, "OptimizedContentProcessor", [], 14]], [], 18], ["__init__", 83, 4, 85, true, "OptimizedContentProcessor", [], 1], ["process_file", 87, 4, 92, true, "OptimizedContentProcessor", [], 2], ["process", 94, 4, 123, true, "OptimizedContentProcessor", [], 14]], "maintainability_index": 31.09, "max_depth": 3}, "imports": ["pathlib", "processors", "asyncio", "datetime", "aiofiles", "dataclasses", "gzip", "typing", "json", "os", "logging", "config"], "functions": [{"name": "__init__", "arguments": ["self"], "decorators": [], "is_async": false}, {"name": "optimize_metrics", "arguments": ["self", "metrics"], "decorators": [], "is_async": false}, {"name": "optimize_function", "arguments": ["self", "func"], "decorators": [], "is_async": false}, {"name": "optimize_class", "arguments": ["self", "cls"], "decorators": [], "is_async": false}, {"name": "optimize_analysis", "arguments": ["self", "analysis"], "decorators": [], "is_async": false}, {"name": "optimize_file_entry", "arguments": ["self", "entry"], "decorators": [], "is_async": false}, {"name": "__init__", "arguments": ["self", "config"], "decorators": [], "is_async": false}], "classes": [{"name": "DescriptiveKeys", "decorators": ["dataclass"]}, {"name": "ContentOptimizer", "methods": [{"name": "__init__", "is_private": true, "is_async": false}, {"name": "optimize_metrics", "is_private": false, "is_async": false}, {"name": "optimize_function", "is_private": false, "is_async": false}, {"name": "optimize_class", "is_private": false, "is_async": false}, {"name": "optimize_analysis", "is_private": false, "is_async": false}, {"name": "optimize_file_entry", "is_private": false, "is_async": false}]}, {"name": "OptimizedContentProcessor", "base_classes": ["ContentProcessor"], "methods": [{"name": "__init__", "is_private": true, "is_async": false}]}]}, "size": 6279, "content": "from dataclasses import dataclass\nfrom pathlib import Path\nfrom typing import Dict, Optional, Any\nimport json\nimport asyncio\nimport aiofiles\nimport logging\nimport gzip\nimport os\nfrom datetime import datetime\nfrom processors.content_processor import ContentProcessor\nfrom config import ProcessorConfig\n\n@dataclass\nclass DescriptiveKeys:\n    \"\"\"Fully descriptive keys for maximum LLM context understanding\"\"\"\n    PATH = 'file_path'\n    TYPE = 'file_type'\n    ANALYSIS = 'analysis'\n    SIZE = 'size'\n    CONTENT = 'content'\n    HASH = 'content_hash'\n    METRICS = 'metrics'\n    IMPORTS = 'imports'\n    FUNCTIONS = 'functions'\n    CLASSES = 'classes'\n    SUCCESS = 'success'\n    LINES_OF_CODE = 'lines_of_code'\n    COMMENT_LINES = 'comment_lines'\n    BLANK_LINES = 'blank_lines'\n    COMPLEXITY = 'complexity'\n    MAINTAINABILITY = 'maintainability_index'\n    MAX_DEPTH = 'max_depth'\n    NAME = 'name'\n    ARGS = 'arguments'\n    DECORATORS = 'decorators'\n    IS_ASYNC = 'is_async'\n    IS_PRIVATE = 'is_private'\n    BASES = 'base_classes'\n    METHODS = 'methods'\n\nclass ContentOptimizer:\n    \"\"\"Processes file analysis output with LLM-friendly descriptive keys\"\"\"\n\n    def __init__(self):\n        self.keys = DescriptiveKeys()\n\n    def optimize_metrics(self, metrics: Dict) -> Dict:\n        \"\"\"Format metrics with descriptive keys\"\"\"\n        return {k: v for (k, v) in {self.keys.LINES_OF_CODE: metrics.get('lines_of_code'), self.keys.COMMENT_LINES: metrics.get('comment_lines'), self.keys.BLANK_LINES: metrics.get('blank_lines'), self.keys.COMPLEXITY: metrics.get('complexity'), self.keys.MAINTAINABILITY: round(metrics.get('maintainability_index', 0), 2), self.keys.MAX_DEPTH: metrics.get('max_depth')}.items() if v is not None}\n\n    def optimize_function(self, func: Dict) -> Dict:\n        \"\"\"Format function data with descriptive keys\"\"\"\n        return {k: v for (k, v) in {self.keys.NAME: func['name'], self.keys.ARGS: func.get('args'), self.keys.DECORATORS: func.get('decorators'), self.keys.IS_ASYNC: func.get('is_async')}.items() if v is not None}\n\n    def optimize_class(self, cls: Dict) -> Dict:\n        \"\"\"Format class data with descriptive keys\"\"\"\n        methods = [{k: v for (k, v) in {self.keys.NAME: m['name'], self.keys.IS_PRIVATE: m.get('is_private'), self.keys.IS_ASYNC: m.get('is_async')}.items() if v is not None} for m in cls['methods']]\n        return {k: v for (k, v) in {self.keys.NAME: cls['name'], self.keys.BASES: cls.get('bases'), self.keys.METHODS: methods, self.keys.DECORATORS: cls.get('decorators')}.items() if v is not None and (not isinstance(v, list) or len(v) > 0)}\n\n    def optimize_analysis(self, analysis: Dict) -> Dict:\n        \"\"\"Format analysis data with descriptive keys\"\"\"\n        if not analysis.get('success', False):\n            return {self.keys.SUCCESS: False}\n        result = {self.keys.SUCCESS: True}\n        if (metrics := analysis.get('metrics')):\n            result[self.keys.METRICS] = self.optimize_metrics(metrics)\n        if (imports := analysis.get('imports')):\n            result[self.keys.IMPORTS] = imports\n        if (functions := analysis.get('functions')):\n            result[self.keys.FUNCTIONS] = [self.optimize_function(f) for f in functions]\n        if (classes := analysis.get('classes')):\n            result[self.keys.CLASSES] = [self.optimize_class(c) for c in classes]\n        return {k: v for (k, v) in result.items() if v is not None}\n\n    def optimize_file_entry(self, entry: Dict) -> Dict:\n        \"\"\"Format file entry with descriptive keys\"\"\"\n        return {k: v for (k, v) in {self.keys.PATH: entry['path'], self.keys.TYPE: entry['type'], self.keys.ANALYSIS: self.optimize_analysis(entry['analysis']), self.keys.SIZE: entry['size'], self.keys.CONTENT: entry['content'], self.keys.HASH: entry['hash']}.items() if v is not None}\n\nclass OptimizedContentProcessor(ContentProcessor):\n    \"\"\"ContentProcessor with LLM-optimized output format\"\"\"\n\n    def __init__(self, config: ProcessorConfig):\n        super().__init__(config)\n        self.optimizer = ContentOptimizer()\n\n    async def process_file(self, file_path: Path):\n        \"\"\"Process a single file with LLM-friendly output.\"\"\"\n        result = await super().process_file(file_path)\n        if result:\n            return self.optimizer.optimize_file_entry(result)\n        return None\n\n    async def process(self) -> dict:\n        \"\"\"Process files with LLM-friendly output format\"\"\"\n        start_time = asyncio.get_event_loop().time()\n        try:\n            self.logger.info(f'Starting processing of {self.config.target_dir}')\n            results = await self.process_directory()\n            output_file = self.config.output_file or f'{self.config.target_dir.name}_context.{self.config.output_format}'\n            if self.config.output_format == 'jsonl':\n                content = []\n                metadata = {'timestamp': datetime.now().isoformat(), 'repository_root': str(self.config.target_dir), 'total_files': len(results), 'statistics': {k: v for (k, v) in self.stats.items() if v is not None}}\n                content.append(json.dumps(metadata))\n                for result in results:\n                    if result:\n                        content.append(json.dumps(result))\n                async with aiofiles.open(output_file, 'w') as f:\n                    await f.write('\\n'.join(content))\n            else:\n                output = {'metadata': {'timestamp': datetime.now().isoformat(), 'repository_root': str(self.config.target_dir), 'statistics': {k: v for (k, v) in self.stats.items() if v is not None}}, 'files': [r for r in results if r]}\n                async with aiofiles.open(output_file, 'w') as f:\n                    await f.write(json.dumps(output))\n            end_time = asyncio.get_event_loop().time()\n            self.stats['processing_time'] = end_time - start_time\n            raw_size = sum((len(str(r).encode('utf-8')) for r in results if r))\n            final_size = os.path.getsize(output_file)\n            reduction = (1 - final_size / raw_size) * 100\n            self.logger.info(f'Output written to {output_file} (Size reduction from null removal: {reduction:.1f}%)')\n            return self.stats\n        except Exception as e:\n            self.logger.error(f'Error during processing: {e}')\n            raise", "content_hash": -7380504636264653818}
{"file_path": "processors/async_processor.py", "file_type": "source", "analysis": {"success": true, "metrics": {"lines_of_code": 131, "comment_lines": 0, "blank_lines": 6, "complexity": [["AsyncContentProcessor", 17, 0, 131, [["__init__", 19, 4, 25, true, "AsyncContentProcessor", [], 1], ["should_process_file", 27, 4, 43, true, "AsyncContentProcessor", [], 7], ["process_file", 45, 4, 90, true, "AsyncContentProcessor", [], 8], ["process_directory", 92, 4, 105, true, "AsyncContentProcessor", [], 7], ["process", 107, 4, 131, true, "AsyncContentProcessor", [], 7]], [], 31], ["__init__", 19, 4, 25, true, "AsyncContentProcessor", [], 1], ["should_process_file", 27, 4, 43, true, "AsyncContentProcessor", [], 7], ["process_file", 45, 4, 90, true, "AsyncContentProcessor", [], 8], ["process_directory", 92, 4, 105, true, "AsyncContentProcessor", [], 7], ["process", 107, 4, 131, true, "AsyncContentProcessor", [], 7]], "maintainability_index": 31.41, "max_depth": 2}, "imports": ["pathlib", "processors", "asyncio", "concurrent", "functools", "datetime", "aiofiles", "exceptions", "tqdm", "typing", "json", "utils", "logging", "config"], "functions": [{"name": "__init__", "arguments": ["self", "config"], "decorators": [], "is_async": false}], "classes": [{"name": "AsyncContentProcessor", "methods": [{"name": "__init__", "is_private": true, "is_async": false}]}]}, "size": 6652, "content": "import asyncio\nimport aiofiles\nfrom pathlib import Path\nfrom typing import Dict, List, Optional, Set\nimport json\nfrom tqdm import tqdm\nimport logging\nfrom concurrent.futures import ProcessPoolExecutor\nfrom functools import partial\nfrom datetime import datetime\nfrom utils.cache_utils import FileAnalysisCache\nfrom utils.file_utils import calculate_file_hash, read_file_safely\nfrom processors.code_analyzer import CodeAnalyzer\nfrom config import ProcessorConfig\nfrom exceptions import FileProcessingError\n\nclass AsyncContentProcessor:\n\n    def __init__(self, config: ProcessorConfig):\n        self.config = config\n        self.logger = logging.getLogger('code_context.processor')\n        self.cache = FileAnalysisCache(Path('.cache'))\n        self.code_analyzer = CodeAnalyzer()\n        self.seen_contents: Set[str] = set()\n        self.stats = {'processed_files': 0, 'skipped_files': 0, 'failed_files': 0, 'total_raw_size': 0, 'total_cleaned_size': 0, 'cache_hits': 0, 'processing_time': 0, 'total_files': 0, 'file_types': {}}\n\n    async def should_process_file(self, file_path: Path) -> bool:\n        \"\"\"Determine if a file should be processed based on configuration.\"\"\"\n        try:\n            if not file_path.is_file():\n                return False\n            if file_path.stat().st_size > self.config.max_file_size:\n                self.logger.debug(f'Skipping {file_path}: exceeds size limit')\n                return False\n            if file_path.suffix.lower() not in self.config.included_extensions:\n                return False\n            rel_path = str(file_path.relative_to(self.config.target_dir))\n            if any((pattern in rel_path for pattern in self.config.excluded_patterns)):\n                return False\n            return True\n        except Exception as e:\n            self.logger.warning(f'Error checking file {file_path}: {e}')\n            return False\n\n    async def process_file(self, file_path: Path) -> Optional[Dict]:\n        \"\"\"Process a single file asynchronously.\"\"\"\n        try:\n            if not await self.should_process_file(file_path):\n                self.stats['skipped_files'] += 1\n                return None\n            raw_size = file_path.stat().st_size\n            self.stats['total_raw_size'] += raw_size\n            file_type = file_path.suffix.lstrip('.')\n            self.stats['file_types'][file_type] = self.stats['file_types'].get(file_type, 0) + 1\n            file_hash = calculate_file_hash(file_path)\n            cached_result = self.cache.get(file_hash)\n            if cached_result is not None:\n                self.stats['cache_hits'] += 1\n                self.stats['total_cleaned_size'] += len(cached_result.get('content', ''))\n                self.stats['processed_files'] += 1\n                return cached_result\n            try:\n                with open(file_path, 'r', encoding='utf-8') as f:\n                    content = f.read()\n            except UnicodeDecodeError:\n                try:\n                    with open(file_path, 'r', encoding='latin-1') as f:\n                        content = f.read()\n                except Exception as e:\n                    self.logger.error(f'Error reading file {file_path}: {e}')\n                    self.stats['failed_files'] += 1\n                    return None\n            if not content:\n                self.stats['skipped_files'] += 1\n                return None\n            loop = asyncio.get_event_loop()\n            with ProcessPoolExecutor() as pool:\n                analysis_func = partial(self.code_analyzer.analyze_code, content, file_type)\n                analysis = await loop.run_in_executor(pool, analysis_func)\n            cleaned_content = self.code_analyzer.clean_content(content, file_type)\n            result = {'path': str(file_path.relative_to(self.config.target_dir)), 'type': file_type, 'analysis': analysis, 'size': len(cleaned_content), 'content': cleaned_content, 'hash': file_hash}\n            self.stats['total_cleaned_size'] += len(cleaned_content)\n            self.stats['processed_files'] += 1\n            if self.config.cache_enabled:\n                self.cache.set(file_hash, result)\n            return result\n        except Exception as e:\n            self.logger.error(f'Error processing {file_path}: {e}')\n            self.stats['failed_files'] += 1\n            return None\n\n    async def process_directory(self) -> List[Dict]:\n        \"\"\"Process all files in the directory asynchronously.\"\"\"\n        files = [f for f in self.config.target_dir.rglob('*') if await self.should_process_file(f)]\n        results = []\n        with tqdm(total=len(files), desc='Processing files') as pbar:\n            chunk_size = 100\n            for i in range(0, len(files), chunk_size):\n                chunk = files[i:i + chunk_size]\n                chunk_tasks = [self.process_file(f) for f in chunk]\n                chunk_results = await asyncio.gather(*chunk_tasks)\n                valid_results = [r for r in chunk_results if r is not None]\n                results.extend(valid_results)\n                pbar.update(len(chunk))\n        return results\n\n    async def process(self) -> dict:\n        \"\"\"Main processing method.\"\"\"\n        start_time = asyncio.get_event_loop().time()\n        try:\n            self.logger.info(f'Starting processing of {self.config.target_dir}')\n            all_files = list(self.config.target_dir.rglob('*'))\n            self.stats['total_files'] = len([f for f in all_files if f.is_file()])\n            results = await self.process_directory()\n            output_file = self.config.output_file or f'{self.config.target_dir.name}_context.{self.config.output_format}'\n            if self.config.output_format == 'jsonl':\n                async with aiofiles.open(output_file, 'w') as f:\n                    metadata = {'timestamp': datetime.now().isoformat(), 'root': str(self.config.target_dir), 'file_count': len(results), 'stats': self.stats}\n                    await f.write(json.dumps(metadata) + '\\n')\n                    for result in results:\n                        await f.write(json.dumps(result) + '\\n')\n            else:\n                output = {'metadata': {'timestamp': datetime.now().isoformat(), 'root': str(self.config.target_dir), 'stats': self.stats}, 'files': results}\n                async with aiofiles.open(output_file, 'w') as f:\n                    await f.write(json.dumps(output, indent=2))\n            end_time = asyncio.get_event_loop().time()\n            self.stats['processing_time'] = end_time - start_time\n            return self.stats\n        except Exception as e:\n            self.logger.error(f'Error during processing: {e}')\n            raise", "content_hash": -7617606947883244090}
{"file_path": "processors/code_analyzer.py", "file_type": "source", "analysis": {"success": true, "metrics": {"lines_of_code": 184, "comment_lines": 0, "blank_lines": 23, "complexity": [["CodeMetrics", 12, 0, 26, [["__post_init__", 24, 4, 26, true, "CodeMetrics", [], 2]], [], 3], ["__post_init__", 24, 4, 26, true, "CodeMetrics", [], 2], ["LanguageAnalyzer", 28, 0, 40, [["analyze", 31, 4, 32, true, "LanguageAnalyzer", [], 1], ["calculate_metrics", 35, 4, 36, true, "LanguageAnalyzer", [], 1], ["clean_content", 39, 4, 40, true, "LanguageAnalyzer", [], 1]], [], 4], ["analyze", 31, 4, 32, true, "LanguageAnalyzer", [], 1], ["calculate_metrics", 35, 4, 36, true, "LanguageAnalyzer", [], 1], ["clean_content", 39, 4, 40, true, "LanguageAnalyzer", [], 1], ["PythonAnalyzer", 42, 0, 121, [["clean_content", 44, 4, 53, true, "PythonAnalyzer", [], 4], ["_analyze_imports", 55, 4, 65, true, "PythonAnalyzer", [], 6], ["_analyze_functions", 67, 4, 74, true, "PythonAnalyzer", [], 5], ["_analyze_classes", 76, 4, 87, true, "PythonAnalyzer", [], 7], ["calculate_metrics", 89, 4, 111, true, "PythonAnalyzer", [["get_depth", 103, 12, 106, false, null, [], 3]], 10], ["analyze", 113, 4, 121, true, "PythonAnalyzer", [], 2]], [], 35], ["clean_content", 44, 4, 53, true, "PythonAnalyzer", [], 4], ["_analyze_imports", 55, 4, 65, true, "PythonAnalyzer", [], 6], ["_analyze_functions", 67, 4, 74, true, "PythonAnalyzer", [], 5], ["_analyze_classes", 76, 4, 87, true, "PythonAnalyzer", [], 7], ["calculate_metrics", 89, 4, 111, true, "PythonAnalyzer", [["get_depth", 103, 12, 106, false, null, [], 3]], 10], ["analyze", 113, 4, 121, true, "PythonAnalyzer", [], 2], ["JavaScriptAnalyzer", 123, 0, 152, [["clean_content", 125, 4, 129, true, "JavaScriptAnalyzer", [], 3], ["calculate_metrics", 131, 4, 140, true, "JavaScriptAnalyzer", [], 3], ["analyze", 142, 4, 152, true, "JavaScriptAnalyzer", [], 2]], [], 9], ["clean_content", 125, 4, 129, true, "JavaScriptAnalyzer", [], 3], ["calculate_metrics", 131, 4, 140, true, "JavaScriptAnalyzer", [], 3], ["analyze", 142, 4, 152, true, "JavaScriptAnalyzer", [], 2], ["CodeAnalyzer", 154, 0, 184, [["__init__", 157, 4, 159, true, "CodeAnalyzer", [], 1], ["get_analyzer", 161, 4, 163, true, "CodeAnalyzer", [], 1], ["analyze_code", 165, 4, 177, true, "CodeAnalyzer", [], 3], ["clean_content", 179, 4, 184, true, "CodeAnalyzer", [], 2]], [], 8], ["__init__", 157, 4, 159, true, "CodeAnalyzer", [], 1], ["get_analyzer", 161, 4, 163, true, "CodeAnalyzer", [], 1], ["analyze_code", 165, 4, 177, true, "CodeAnalyzer", [], 3], ["clean_content", 179, 4, 184, true, "CodeAnalyzer", [], 2]], "maintainability_index": 30.76, "max_depth": 4}, "imports": ["re", "abc", "dataclasses", "typing", "radon", "ast", "logging"], "functions": [{"name": "__post_init__", "arguments": ["self"], "decorators": [], "is_async": false}, {"name": "analyze", "arguments": ["self", "content"], "decorators": ["abstractmethod"], "is_async": false}, {"name": "calculate_metrics", "arguments": ["self", "content"], "decorators": ["abstractmethod"], "is_async": false}, {"name": "clean_content", "arguments": ["self", "content"], "decorators": ["abstractmethod"], "is_async": false}, {"name": "clean_content", "arguments": ["self", "content"], "decorators": [], "is_async": false}, {"name": "_analyze_imports", "arguments": ["self", "tree"], "decorators": [], "is_async": false}, {"name": "_analyze_functions", "arguments": ["self", "tree"], "decorators": [], "is_async": false}, {"name": "_analyze_classes", "arguments": ["self", "tree"], "decorators": [], "is_async": false}, {"name": "calculate_metrics", "arguments": ["self", "content"], "decorators": [], "is_async": false}, {"name": "analyze", "arguments": ["self", "content"], "decorators": [], "is_async": false}, {"name": "clean_content", "arguments": ["self", "content"], "decorators": [], "is_async": false}, {"name": "calculate_metrics", "arguments": ["self", "content"], "decorators": [], "is_async": false}, {"name": "analyze", "arguments": ["self", "content"], "decorators": [], "is_async": false}, {"name": "__init__", "arguments": ["self"], "decorators": [], "is_async": false}, {"name": "get_analyzer", "arguments": ["self", "file_type"], "decorators": [], "is_async": false}, {"name": "analyze_code", "arguments": ["self", "content", "file_type"], "decorators": [], "is_async": false}, {"name": "clean_content", "arguments": ["self", "content", "file_type"], "decorators": [], "is_async": false}, {"name": "get_depth", "arguments": ["node", "current_depth"], "decorators": [], "is_async": false}], "classes": [{"name": "CodeMetrics", "methods": [{"name": "__post_init__", "is_private": true, "is_async": false}], "decorators": ["dataclass"]}, {"name": "LanguageAnalyzer", "base_classes": ["ABC"], "methods": [{"name": "analyze", "is_private": false, "is_async": false}, {"name": "calculate_metrics", "is_private": false, "is_async": false}, {"name": "clean_content", "is_private": false, "is_async": false}]}, {"name": "PythonAnalyzer", "base_classes": ["LanguageAnalyzer"], "methods": [{"name": "clean_content", "is_private": false, "is_async": false}, {"name": "_analyze_imports", "is_private": true, "is_async": false}, {"name": "_analyze_functions", "is_private": true, "is_async": false}, {"name": "_analyze_classes", "is_private": true, "is_async": false}, {"name": "calculate_metrics", "is_private": false, "is_async": false}, {"name": "analyze", "is_private": false, "is_async": false}]}, {"name": "JavaScriptAnalyzer", "base_classes": ["LanguageAnalyzer"], "methods": [{"name": "clean_content", "is_private": false, "is_async": false}, {"name": "calculate_metrics", "is_private": false, "is_async": false}, {"name": "analyze", "is_private": false, "is_async": false}]}, {"name": "CodeAnalyzer", "methods": [{"name": "__init__", "is_private": true, "is_async": false}, {"name": "get_analyzer", "is_private": false, "is_async": false}, {"name": "analyze_code", "is_private": false, "is_async": false}, {"name": "clean_content", "is_private": false, "is_async": false}]}]}, "size": 8726, "content": "import ast\nimport re\nfrom typing import Dict, Any, List, Optional\nimport logging\nfrom dataclasses import dataclass\nfrom abc import ABC, abstractmethod\nimport radon.metrics\nimport radon.complexity\nfrom typing import Set\n\n@dataclass\nclass CodeMetrics:\n    lines_of_code: int = 0\n    comment_lines: int = 0\n    blank_lines: int = 0\n    complexity: int = 0\n    functions_count: int = 0\n    classes_count: int = 0\n    max_depth: int = 0\n    dependencies: Set[str] = None\n    maintainability_index: float = 0.0\n    cognitive_complexity: int = 0\n\n    def __post_init__(self):\n        if self.dependencies is None:\n            self.dependencies = set()\n\nclass LanguageAnalyzer(ABC):\n\n    @abstractmethod\n    def analyze(self, content: str) -> Dict[str, Any]:\n        pass\n\n    @abstractmethod\n    def calculate_metrics(self, content: str) -> CodeMetrics:\n        pass\n\n    @abstractmethod\n    def clean_content(self, content: str) -> str:\n        pass\n\nclass PythonAnalyzer(LanguageAnalyzer):\n\n    def clean_content(self, content: str) -> str:\n        \"\"\"Remove comments and normalize whitespace in Python code.\"\"\"\n        try:\n            tree = ast.parse(content)\n            return ast.unparse(tree)\n        except:\n            content = re.sub('#.*$', '', content, flags=re.MULTILINE)\n            content = re.sub('\"\"\"[\\\\s\\\\S]*?\"\"\"', '', content)\n            content = re.sub(\"'''[\\\\s\\\\S]*?'''\", '', content)\n            return '\\n'.join((line for line in content.splitlines() if line.strip()))\n\n    def _analyze_imports(self, tree: ast.AST) -> Set[str]:\n        \"\"\"Analyze imports in Python code.\"\"\"\n        imports = set()\n        for node in ast.walk(tree):\n            if isinstance(node, ast.Import):\n                for name in node.names:\n                    imports.add(name.name.split('.')[0])\n            elif isinstance(node, ast.ImportFrom):\n                if node.module:\n                    imports.add(node.module.split('.')[0])\n        return imports\n\n    def _analyze_functions(self, tree: ast.AST) -> List[Dict[str, Any]]:\n        \"\"\"Analyze functions in Python code.\"\"\"\n        functions = []\n        for node in ast.walk(tree):\n            if isinstance(node, ast.FunctionDef):\n                func_info = {'name': node.name, 'args': [arg.arg for arg in node.args.args], 'decorators': [ast.unparse(d) for d in node.decorator_list], 'is_async': isinstance(node, ast.AsyncFunctionDef), 'complexity': radon.complexity.cc_visit(node)}\n                functions.append(func_info)\n        return functions\n\n    def _analyze_classes(self, tree: ast.AST) -> List[Dict[str, Any]]:\n        \"\"\"Analyze classes in Python code.\"\"\"\n        classes = []\n        for node in ast.walk(tree):\n            if isinstance(node, ast.ClassDef):\n                methods = []\n                for child in node.body:\n                    if isinstance(child, ast.FunctionDef):\n                        methods.append({'name': child.name, 'is_private': child.name.startswith('_'), 'is_async': isinstance(child, ast.AsyncFunctionDef)})\n                class_info = {'name': node.name, 'bases': [ast.unparse(base) for base in node.bases], 'methods': methods, 'decorators': [ast.unparse(d) for d in node.decorator_list]}\n                classes.append(class_info)\n        return classes\n\n    def calculate_metrics(self, content: str) -> CodeMetrics:\n        \"\"\"Calculate code metrics for Python code.\"\"\"\n        metrics = CodeMetrics()\n        lines = content.splitlines()\n        metrics.lines_of_code = len(lines)\n        metrics.blank_lines = sum((1 for line in lines if not line.strip()))\n        metrics.comment_lines = sum((1 for line in lines if line.strip().startswith('#')))\n        try:\n            tree = ast.parse(content)\n            metrics.complexity = radon.complexity.cc_visit(tree)\n            metrics.maintainability_index = radon.metrics.mi_visit(content, True)\n            metrics.functions_count = len([node for node in ast.walk(tree) if isinstance(node, ast.FunctionDef)])\n            metrics.classes_count = len([node for node in ast.walk(tree) if isinstance(node, ast.ClassDef)])\n\n            def get_depth(node: ast.AST, current_depth: int=0) -> int:\n                if isinstance(node, (ast.For, ast.While, ast.If, ast.With)):\n                    current_depth += 1\n                return max([current_depth] + [get_depth(child, current_depth) for child in ast.iter_child_nodes(node)])\n            metrics.max_depth = get_depth(tree)\n            metrics.dependencies = self._analyze_imports(tree)\n        except Exception as e:\n            logging.error(f'Error calculating metrics: {e}')\n        return metrics\n\n    def analyze(self, content: str) -> Dict[str, Any]:\n        \"\"\"Perform comprehensive analysis of Python code.\"\"\"\n        try:\n            tree = ast.parse(content)\n            metrics = self.calculate_metrics(content)\n            return {'metrics': {'lines_of_code': metrics.lines_of_code, 'comment_lines': metrics.comment_lines, 'blank_lines': metrics.blank_lines, 'complexity': metrics.complexity, 'maintainability_index': metrics.maintainability_index, 'max_depth': metrics.max_depth}, 'imports': list(metrics.dependencies), 'functions': self._analyze_functions(tree), 'classes': self._analyze_classes(tree), 'success': True}\n        except Exception as e:\n            logging.error(f'Error analyzing Python code: {e}')\n            return {'success': False, 'error': str(e)}\n\nclass JavaScriptAnalyzer(LanguageAnalyzer):\n\n    def clean_content(self, content: str) -> str:\n        \"\"\"Remove comments and normalize whitespace in JavaScript code.\"\"\"\n        content = re.sub('//.*$', '', content, flags=re.MULTILINE)\n        content = re.sub('/\\\\*[\\\\s\\\\S]*?\\\\*/', '', content)\n        return '\\n'.join((line for line in content.splitlines() if line.strip()))\n\n    def calculate_metrics(self, content: str) -> CodeMetrics:\n        metrics = CodeMetrics()\n        lines = content.splitlines()\n        metrics.lines_of_code = len(lines)\n        metrics.blank_lines = sum((1 for line in lines if not line.strip()))\n        control_structures = len(re.findall('\\\\b(if|for|while|switch)\\\\b', content))\n        metrics.complexity = control_structures\n        metrics.functions_count = len(re.findall('\\\\bfunction\\\\s+\\\\w+\\\\s*\\\\(', content))\n        metrics.classes_count = len(re.findall('\\\\bclass\\\\s+\\\\w+\\\\b', content))\n        return metrics\n\n    def analyze(self, content: str) -> Dict[str, Any]:\n        \"\"\"Analyze JavaScript code.\"\"\"\n        try:\n            cleaned_content = self.clean_content(content)\n            metrics = self.calculate_metrics(cleaned_content)\n            imports = re.findall('import\\\\s+.*?from\\\\s+[\\\\\\'\"]([^\\\\\\'\"]+)[\\\\\\'\"]', content)\n            exports = re.findall('export\\\\s+(?:default\\\\s+)?(?:class|function|const|let|var)\\\\s+(\\\\w+)', content)\n            return {'metrics': {'lines_of_code': metrics.lines_of_code, 'blank_lines': metrics.blank_lines, 'complexity': metrics.complexity, 'functions_count': metrics.functions_count, 'classes_count': metrics.classes_count}, 'imports': imports, 'exports': exports, 'success': True}\n        except Exception as e:\n            logging.error(f'Error analyzing JavaScript code: {e}')\n            return {'success': False, 'error': str(e)}\n\nclass CodeAnalyzer:\n    \"\"\"Main analyzer class that delegates to language-specific analyzers.\"\"\"\n\n    def __init__(self):\n        self.analyzers = {'py': PythonAnalyzer(), 'js': JavaScriptAnalyzer(), 'jsx': JavaScriptAnalyzer(), 'ts': JavaScriptAnalyzer(), 'tsx': JavaScriptAnalyzer()}\n        self.logger = logging.getLogger('code_context.analyzer')\n\n    def get_analyzer(self, file_type: str) -> Optional[LanguageAnalyzer]:\n        \"\"\"Get the appropriate analyzer for a file type.\"\"\"\n        return self.analyzers.get(file_type.lower())\n\n    def analyze_code(self, content: str, file_type: str) -> Dict[str, Any]:\n        \"\"\"Analyze code content with the appropriate analyzer.\"\"\"\n        analyzer = self.get_analyzer(file_type)\n        if not analyzer:\n            self.logger.warning(f'No analyzer available for file type: {file_type}')\n            return {'success': False, 'error': f'Unsupported file type: {file_type}'}\n        try:\n            result = analyzer.analyze(content)\n            result['file_type'] = file_type\n            return result\n        except Exception as e:\n            self.logger.error(f'Error analyzing {file_type} code: {e}')\n            return {'success': False, 'error': str(e)}\n\n    def clean_content(self, content: str, file_type: str) -> str:\n        \"\"\"Clean code content with the appropriate analyzer.\"\"\"\n        analyzer = self.get_analyzer(file_type)\n        if not analyzer:\n            return content\n        return analyzer.clean_content(content)", "content_hash": -501676273423584873}