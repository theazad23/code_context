{"timestamp": "2024-11-08T23:53:00.396159", "repository_root": "/home/anthony_z/ai/code_context", "total_files": 10, "statistics": {"processed_files": 10, "skipped_files": 2, "failed_files": 0, "total_raw_size": 32500, "total_cleaned_size": 27684, "processing_time": 0, "total_files": 12, "file_types": {"py": 10}, "failed_files_info": []}}
{"path": "exceptions.py", "type": "py", "analysis": {"success": true, "metrics": {"lines_of_code": 11, "comment_lines": 0, "blank_lines": 2, "complexity": 1, "maintainability_index": 100.0, "max_depth": 0}, "imports": [], "functions": [], "classes": [{"name": "CodeContextError", "methods": [], "base_classes": ["Exception"]}, {"name": "FileProcessingError", "methods": [], "base_classes": ["CodeContextError"]}, {"name": "ConfigurationError", "methods": [], "base_classes": ["CodeContextError"]}]}, "size": 324, "content": "class CodeContextError(Exception):\n    \"\"\"Base exception for code context related errors.\"\"\"\n    pass\n\nclass FileProcessingError(CodeContextError):\n    \"\"\"Raised when there's an error processing a file.\"\"\"\n    pass\n\nclass ConfigurationError(CodeContextError):\n    \"\"\"Raised when there's an invalid configuration.\"\"\"\n    pass", "hash": "831a8e30f73239747c7b7b892b74d82af0907f1c500d30f3518f2817563fbbd2"}
{"path": "utils.py", "type": "py", "analysis": {"success": true, "metrics": {"lines_of_code": 33, "comment_lines": 0, "blank_lines": 3, "complexity": 9, "maintainability_index": 100.0, "max_depth": 2}, "imports": ["logging", "hashlib", "pathlib", "typing", "chardet"], "functions": [{"name": "get_file_encoding", "arguments": ["file_path"], "decorators": [], "is_async": false}, {"name": "read_file_safely", "arguments": ["file_path", "logger"], "decorators": [], "is_async": false}, {"name": "calculate_file_hash", "arguments": ["file_path"], "decorators": [], "is_async": false}], "classes": []}, "size": 1125, "content": "import hashlib\nimport logging\nfrom pathlib import Path\nimport chardet\nfrom typing import Optional\n\ndef get_file_encoding(file_path: Path) -> str:\n    with open(file_path, 'rb') as f:\n        raw_data = f.read()\n        result = chardet.detect(raw_data)\n        return result['encoding'] or 'utf-8'\n\ndef read_file_safely(file_path: Path, logger: logging.Logger) -> Optional[str]:\n    try:\n        encoding = get_file_encoding(file_path)\n        with open(file_path, 'r', encoding=encoding) as f:\n            return f.read()\n    except UnicodeDecodeError:\n        for enc in ['utf-8', 'latin-1', 'cp1252']:\n            try:\n                with open(file_path, 'r', encoding=enc) as f:\n                    return f.read()\n            except UnicodeDecodeError:\n                continue\n    logger.error(f'Failed to read {file_path} with any encoding')\n    return None\n\ndef calculate_file_hash(file_path: Path) -> str:\n    sha256_hash = hashlib.sha256()\n    with open(file_path, 'rb') as f:\n        for byte_block in iter(lambda : f.read(4096), b''):\n            sha256_hash.update(byte_block)\n    return sha256_hash.hexdigest()", "hash": "7c939603f48f1f9b033be3dce10c9c6c40857c555ab5dfc74f4a66b1b6802856"}
{"path": "cli.py", "type": "py", "analysis": {"success": true, "metrics": {"lines_of_code": 71, "comment_lines": 0, "blank_lines": 4, "complexity": 13, "maintainability_index": 100.0, "max_depth": 2}, "imports": ["logging", "processors", "argparse", "pathlib", "sys", "config", "asyncio", "rich"], "functions": [{"name": "format_size", "arguments": ["size_in_bytes"], "decorators": [], "is_async": false}, {"name": "parse_args", "arguments": [], "decorators": [], "is_async": false}, {"name": "display_summary", "arguments": ["stats"], "decorators": [], "is_async": false}], "classes": []}, "size": 3336, "content": "import asyncio\nimport argparse\nimport logging\nimport sys\nfrom pathlib import Path\nfrom rich.console import Console\nfrom rich.table import Table\nfrom rich.panel import Panel\nconsole = Console()\n\ndef format_size(size_in_bytes: int) -> str:\n    for unit in ['B', 'KB', 'MB', 'GB']:\n        if size_in_bytes < 1024:\n            return f'{size_in_bytes:.2f} {unit}'\n        size_in_bytes /= 1024\n    return f'{size_in_bytes:.2f} TB'\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description='Code Context Generator')\n    parser.add_argument('path', type=str, help='Path to code directory')\n    parser.add_argument('-o', '--output', type=str, help='Output file path')\n    parser.add_argument('-f', '--format', choices=['jsonl', 'json'], default='jsonl')\n    parser.add_argument('--include-tests', action='store_true')\n    parser.add_argument('--max-size', type=int, default=1024 * 1024)\n    parser.add_argument('-v', '--verbose', action='store_true')\n    args = parser.parse_args()\n    path = Path(args.path)\n    if not path.exists() or not path.is_dir():\n        console.print(f'[red]Error:[/red] Invalid directory: {path}')\n        sys.exit(1)\n    return args\n\ndef display_summary(stats: dict):\n    table = Table(title='Processing Summary', show_header=True, header_style='bold blue')\n    table.add_column('Metric', style='cyan')\n    table.add_column('Value', justify='right', style='green')\n    metrics = [('Files Processed', stats['processed_files']), ('Files Skipped', stats['skipped_files']), ('Files Failed', stats['failed_files']), ('Total Files', stats['total_files']), ('Original Size', format_size(stats['total_raw_size'])), ('Cleaned Size', format_size(stats['total_cleaned_size'])), ('Processing Time', f\"{stats['processing_time']:.2f} seconds\")]\n    for (label, value) in metrics:\n        table.add_row(label, str(value))\n    if stats['total_raw_size'] > 0:\n        reduction = (1 - stats['total_cleaned_size'] / stats['total_raw_size']) * 100\n        table.add_row('Size Reduction', f'{reduction:.2f}%')\n    console.print('\\n')\n    console.print(table)\n    console.print('\\n')\n\nasync def main():\n    try:\n        console.print(Panel.fit('[bold blue]Code Context Generator[/bold blue]', border_style='blue'))\n        args = parse_args()\n        from config import ProcessorConfig\n        from processors.optimized_processor import OptimizedContentProcessor\n        config = ProcessorConfig(target_dir=args.path, output_file=args.output, output_format=args.format, include_tests=args.include_tests, max_file_size=args.max_size, verbose=args.verbose)\n        processor = OptimizedContentProcessor(config)\n        with console.status('[bold green]Processing files...') as status:\n            stats = await processor.process()\n        display_summary(stats)\n        if config.output_file:\n            console.print(f'Results written to: [bold]{config.output_file}[/bold]')\n    except KeyboardInterrupt:\n        console.print('\\n[yellow]Process interrupted by user[/yellow]')\n        sys.exit(1)\n    except Exception as e:\n        console.print(f'\\n[red]Error:[/red] {str(e)}')\n        if args.verbose:\n            console.print_exception()\n        sys.exit(1)\nif __name__ == '__main__':\n    if sys.platform == 'win32':\n        asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())\n    asyncio.run(main())", "hash": "a8decac2f122b9d1693abfa1d7772c94f167c614fc0a35b73ecfb4fda43f01a5"}
{"path": "config.py", "type": "py", "analysis": {"success": true, "metrics": {"lines_of_code": 24, "comment_lines": 0, "blank_lines": 2, "complexity": 2, "maintainability_index": 100.0, "max_depth": 1}, "imports": ["pathlib", "dataclasses", "typing"], "functions": [{"name": "__post_init__", "arguments": ["self"], "decorators": [], "is_async": false}], "classes": [{"name": "ProcessorConfig", "methods": [{"name": "__post_init__", "arguments": [], "decorators": [], "is_async": false}], "base_classes": []}]}, "size": 908, "content": "from dataclasses import dataclass\nfrom typing import Set, Dict, Optional, List\nfrom pathlib import Path\n\n@dataclass\nclass ProcessorConfig:\n    target_dir: Path\n    output_file: Optional[str] = None\n    include_tests: bool = False\n    output_format: str = 'jsonl'\n    max_file_size: int = 1024 * 1024\n    worker_count: int = 4\n    cache_enabled: bool = True\n    verbose: bool = False\n    optimize: bool = True\n    excluded_patterns: Set[str] = None\n    included_extensions: Set[str] = None\n\n    def __post_init__(self):\n        self.target_dir = Path(self.target_dir)\n        if self.excluded_patterns is None:\n            self.excluded_patterns = {'node_modules', '.git', 'venv', '__pycache__', 'dist', 'build', '.env', '.pytest_cache'}\n        if self.included_extensions is None:\n            self.included_extensions = {'.py', '.js', '.jsx', '.ts', '.tsx', '.java', '.cpp', '.c', '.h', '.cs', '.go', '.rb'}", "hash": "9d7236ed1f3f1a13f9eb54d922500db12f12e324cb4689be2b3bcede695cab78"}
{"path": "__init__.py", "type": "py", "analysis": {"success": true, "metrics": {"lines_of_code": 3, "comment_lines": 0, "blank_lines": 0, "complexity": 1, "maintainability_index": 100.0, "max_depth": 0}, "imports": ["main"], "functions": [], "classes": []}, "size": 87, "content": "from .main import generate_context\n__version__ = '1.0.0'\n__all__ = ['generate_context']", "hash": "fe8c1721cd64af1c091f1d0bd8a9f8838b89d3c0856ed2b8fa3230aaa8cb2cc0"}
{"path": "parsers/gitignore_parser.py", "type": "py", "analysis": {"success": true, "metrics": {"lines_of_code": 63, "comment_lines": 0, "blank_lines": 7, "complexity": 18, "maintainability_index": 100.0, "max_depth": 4}, "imports": ["logging", "re", "pathlib"], "functions": [{"name": "__init__", "arguments": ["self", "pattern"], "decorators": [], "is_async": false}, {"name": "_glob_to_regex", "arguments": ["self", "pattern"], "decorators": [], "is_async": false}, {"name": "matches", "arguments": ["self", "path"], "decorators": [], "is_async": false}, {"name": "__init__", "arguments": ["self", "gitignore_path"], "decorators": [], "is_async": false}, {"name": "is_ignored", "arguments": ["self", "path"], "decorators": [], "is_async": false}], "classes": [{"name": "GitignorePattern", "methods": [{"name": "__init__", "arguments": [], "decorators": [], "is_async": false}, {"name": "_glob_to_regex", "arguments": [], "decorators": [], "is_async": false}, {"name": "matches", "arguments": [], "decorators": [], "is_async": false}], "base_classes": []}, {"name": "GitignoreParser", "methods": [{"name": "__init__", "arguments": [], "decorators": [], "is_async": false}, {"name": "is_ignored", "arguments": [], "decorators": [], "is_async": false}], "base_classes": []}]}, "size": 2099, "content": "import re\nimport logging\nfrom pathlib import Path\nlogger = logging.getLogger(__name__)\n\nclass GitignorePattern:\n\n    def __init__(self, pattern: str):\n        self.raw_pattern = pattern\n        self.is_negated = pattern.startswith('!')\n        self.is_directory_only = pattern.endswith('/')\n        pattern = pattern[1:] if self.is_negated else pattern\n        pattern = pattern.rstrip('/')\n        self.regex = self._glob_to_regex(pattern)\n\n    def _glob_to_regex(self, pattern: str) -> re.Pattern:\n        regex = ''\n        if not pattern.startswith('/'):\n            regex = '(?:.+/)?'\n        for c in pattern:\n            if c == '*':\n                regex += '[^/]*'\n            elif c == '?':\n                regex += '[^/]'\n            elif c in '.[]()+^${}|':\n                regex += f'\\\\{c}'\n            else:\n                regex += c\n        regex = f'^{regex}(?:/.*)?$' if self.is_directory_only else f'^{regex}$'\n        return re.compile(regex)\n\n    def matches(self, path: str) -> bool:\n        return bool(self.regex.match(path))\n\nclass GitignoreParser:\n\n    def __init__(self, gitignore_path: Path):\n        self.patterns = []\n        self.negated_patterns = []\n        if not gitignore_path.exists():\n            logger.warning(f'No .gitignore found at {gitignore_path}')\n            return\n        with open(gitignore_path, 'r') as f:\n            for line in f:\n                line = line.strip()\n                if line and (not line.startswith('#')):\n                    pattern = GitignorePattern(line)\n                    if pattern.is_negated:\n                        self.negated_patterns.append(pattern)\n                    else:\n                        self.patterns.append(pattern)\n\n    def is_ignored(self, path: str) -> bool:\n        path = path.replace('\\\\', '/')\n        if path.startswith('./'):\n            path = path[2:]\n        for pattern in self.negated_patterns:\n            if pattern.matches(path):\n                return False\n        for pattern in self.patterns:\n            if pattern.matches(path):\n                return True\n        return False", "hash": "9c83d2b055d42d8723b2629e315470ca9274b93a6f628d65fc60479991e334f0"}
{"path": "parsers/file_parser.py", "type": "py", "analysis": {"success": true, "metrics": {"lines_of_code": 46, "comment_lines": 0, "blank_lines": 5, "complexity": 11, "maintainability_index": 100.0, "max_depth": 2}, "imports": ["pathlib", "logging", "parsers", "fnmatch"], "functions": [{"name": "__init__", "arguments": ["self", "include_tests"], "decorators": [], "is_async": false}, {"name": "is_test_file", "arguments": ["self", "file_path"], "decorators": [], "is_async": false}, {"name": "should_process_file", "arguments": ["self", "file_path", "gitignore_parser"], "decorators": [], "is_async": false}, {"name": "get_file_type", "arguments": ["self", "file_path"], "decorators": [], "is_async": false}], "classes": [{"name": "FileParser", "methods": [{"name": "__init__", "arguments": [], "decorators": [], "is_async": false}, {"name": "is_test_file", "arguments": [], "decorators": [], "is_async": false}, {"name": "should_process_file", "arguments": [], "decorators": [], "is_async": false}, {"name": "get_file_type", "arguments": [], "decorators": [], "is_async": false}], "base_classes": []}]}, "size": 2605, "content": "from pathlib import Path\nfrom fnmatch import fnmatch\nimport logging\nfrom parsers.gitignore_parser import GitignoreParser\nlogger = logging.getLogger(__name__)\n\nclass FileParser:\n\n    def __init__(self, include_tests: bool=False):\n        self.include_tests = include_tests\n        self.test_patterns = {'**/test_*.py', '**/tests/*.py', '**/tests/**/*.py', '**/*_test.py', '**/test/*.py', '**/test/**/*.py', '**/*.spec.js', '**/*.test.js', '**/test/*.js', '**/tests/*.js', '**/__tests__/**'}\n        self.ignore_patterns = {'node_modules', '.git', 'venv', '__pycache__', 'dist', 'build', '.min.', 'vendor/', '.DS_Store', '.env', '.coverage', 'bundle.', 'package-lock.json', 'README.md', '.pytest_cache', 'htmlcov', '.coverage', 'test-results', '*.pyc', '*.pyo'}\n        self.file_categories = {'source': {'.py', '.js', '.jsx', '.ts', '.tsx', '.java', '.cpp', '.c', '.h', '.cs', '.go', '.rb'}, 'config': {'.json', '.yaml', '.yml', '.toml', '.ini', '.xml'}, 'doc': {'.md', '.rst', '.txt'}, 'style': {'.css', '.scss', '.sass', '.less'}, 'template': {'.html', '.jinja', '.jinja2', '.tmpl'}, 'shell': {'.sh', '.bash', '.zsh', '.fish'}}\n\n    def is_test_file(self, file_path: str) -> bool:\n        normalized_path = str(file_path).replace('\\\\', '/')\n        return any((fnmatch(normalized_path, pattern) for pattern in self.test_patterns))\n\n    def should_process_file(self, file_path: Path, gitignore_parser) -> bool:\n        try:\n            if not file_path.exists() or not file_path.is_file():\n                return False\n            rel_path = str(file_path)\n            rel_path_str = rel_path.replace('\\\\', '/')\n            if gitignore_parser and gitignore_parser.is_ignored(rel_path_str):\n                return False\n            if any((p in rel_path for p in self.ignore_patterns)):\n                return False\n            is_test = self.is_test_file(rel_path_str)\n            if is_test and (not self.include_tests):\n                return False\n            valid_extensions = {ext for exts in self.file_categories.values() for ext in exts}\n            if file_path.suffix.lower() not in valid_extensions:\n                return False\n            return True\n        except Exception as e:\n            logger.error(f'Error checking file {file_path}: {e}')\n            return False\n\n    def get_file_type(self, file_path: Path) -> str:\n        \"\"\"Get the category of the file based on its extension.\"\"\"\n        ext = file_path.suffix.lower()\n        for (category, extensions) in self.file_categories.items():\n            if ext in extensions:\n                return category\n        return 'unknown'", "hash": "e5827c3c1fb1c08ad362bff41d527a36e90f2ca59042427820c3311c6e383f14"}
{"path": "processors/optimized_processor.py", "type": "py", "analysis": {"success": true, "metrics": {"lines_of_code": 143, "comment_lines": 0, "blank_lines": 5, "complexity": 29, "maintainability_index": 100.0, "max_depth": 3}, "imports": ["logging", "datetime", "utils", "traceback", "pathlib", "config", "code_analyzer", "typing", "asyncio", "rich", "json", "aiofiles"], "functions": [{"name": "__init__", "arguments": ["self", "config"], "decorators": [], "is_async": false}], "classes": [{"name": "OptimizedContentProcessor", "methods": [{"name": "__init__", "arguments": [], "decorators": [], "is_async": false}], "base_classes": []}]}, "size": 8030, "content": "import asyncio\nimport json\nfrom datetime import datetime\nfrom pathlib import Path\nfrom typing import Dict, List, Optional\nimport logging\nimport aiofiles\nfrom rich.progress import Progress\nfrom config import ProcessorConfig\nfrom .code_analyzer import CodeAnalyzer\nfrom utils import read_file_safely, calculate_file_hash\nimport traceback\n\nclass OptimizedContentProcessor:\n    \"\"\"Processes source code files with optimized performance and memory usage.\"\"\"\n\n    def __init__(self, config: ProcessorConfig):\n        self.config = config\n        self.analyzer = CodeAnalyzer()\n        self.logger = logging.getLogger(__name__)\n        if config.verbose:\n            self.logger.setLevel(logging.DEBUG)\n        self.stats = {'processed_files': 0, 'skipped_files': 0, 'failed_files': 0, 'total_raw_size': 0, 'total_cleaned_size': 0, 'processing_time': 0, 'total_files': 0, 'file_types': {}, 'failed_files_info': []}\n\n    async def should_process_file(self, file_path: Path) -> bool:\n        \"\"\"Determine if a file should be processed.\"\"\"\n        try:\n            if not file_path.is_file():\n                self.logger.debug(f'Skipping {file_path}: not a file')\n                return False\n            size = file_path.stat().st_size\n            if size > self.config.max_file_size:\n                self.logger.debug(f'Skipping {file_path}: size {size} exceeds limit {self.config.max_file_size}')\n                return False\n            if file_path.suffix.lower() not in self.config.included_extensions:\n                self.logger.debug(f'Skipping {file_path}: extension {file_path.suffix} not in {self.config.included_extensions}')\n                return False\n            rel_path = str(file_path.relative_to(self.config.target_dir))\n            if any((pattern in rel_path for pattern in self.config.excluded_patterns)):\n                self.logger.debug(f'Skipping {file_path}: matches excluded pattern')\n                return False\n            self.logger.debug(f'Will process {file_path}')\n            return True\n        except Exception as e:\n            self.logger.warning(f'Error checking file {file_path}: {e}')\n            return False\n\n    async def process_file(self, file_path: Path) -> Optional[Dict]:\n        \"\"\"Process a single file.\"\"\"\n        try:\n            if not await self.should_process_file(file_path):\n                self.stats['skipped_files'] += 1\n                return None\n            self.logger.debug(f'Starting to process {file_path}')\n            raw_size = file_path.stat().st_size\n            if raw_size == 0:\n                self.logger.debug(f'Skipping empty file: {file_path}')\n                self.stats['skipped_files'] += 1\n                return None\n            self.stats['total_raw_size'] += raw_size\n            content = read_file_safely(file_path, self.logger)\n            if not content or not content.strip():\n                self.logger.debug(f'Skipping empty file: {file_path}')\n                self.stats['skipped_files'] += 1\n                return None\n            file_type = file_path.suffix.lstrip('.')\n            try:\n                cleaned_content = self.analyzer.clean_content(content, file_type)\n                if not cleaned_content.strip():\n                    self.logger.debug(f'Skipping {file_path}: empty after cleaning')\n                    self.stats['skipped_files'] += 1\n                    return None\n            except Exception as e:\n                self.logger.error(f'Failed to clean content of {file_path}: {e}')\n                self.stats['failed_files'] += 1\n                self.stats['failed_files_info'].append({'file': str(file_path), 'error': f'Content cleaning error: {str(e)}', 'traceback': traceback.format_exc()})\n                return None\n            cleaned_size = len(cleaned_content.encode('utf-8'))\n            self.stats['total_cleaned_size'] += cleaned_size\n            self.stats['file_types'][file_type] = self.stats['file_types'].get(file_type, 0) + 1\n            try:\n                analysis = self.analyzer.analyze_code(cleaned_content, file_type)\n                if not analysis.get('success', False):\n                    error_msg = analysis.get('error', 'Unknown analysis error')\n                    self.logger.error(f'Analysis failed for {file_path}: {error_msg}')\n                    self.stats['failed_files'] += 1\n                    self.stats['failed_files_info'].append({'file': str(file_path), 'error': f'Analysis error: {error_msg}'})\n                    return None\n            except Exception as e:\n                self.logger.error(f'Exception during analysis of {file_path}: {e}')\n                self.stats['failed_files'] += 1\n                self.stats['failed_files_info'].append({'file': str(file_path), 'error': f'Analysis exception: {str(e)}', 'traceback': traceback.format_exc()})\n                return None\n            self.stats['processed_files'] += 1\n            self.logger.debug(f'Successfully processed {file_path}')\n            return {'path': str(file_path.relative_to(self.config.target_dir)), 'type': file_type, 'analysis': analysis, 'size': cleaned_size, 'content': cleaned_content, 'hash': calculate_file_hash(file_path)}\n        except Exception as e:\n            self.logger.error(f'Unexpected error processing {file_path}: {e}')\n            self.logger.error(traceback.format_exc())\n            self.stats['failed_files'] += 1\n            self.stats['failed_files_info'].append({'file': str(file_path), 'error': f'Unexpected error: {str(e)}', 'traceback': traceback.format_exc()})\n            return None\n\n    async def process(self) -> dict:\n        \"\"\"Process all files in the target directory.\"\"\"\n        start_time = asyncio.get_event_loop().time()\n        try:\n            self.logger.info(f'Starting processing of {self.config.target_dir}')\n            files = [f for f in self.config.target_dir.rglob('*') if f.is_file() and await self.should_process_file(f)]\n            self.stats['total_files'] = len(files)\n            results = []\n            with Progress() as progress:\n                task = progress.add_task('[cyan]Processing files...', total=len(files))\n                for file_path in files:\n                    result = await self.process_file(file_path)\n                    if result:\n                        results.append(result)\n                    progress.update(task, advance=1)\n            output_file = self.config.output_file or f'{self.config.target_dir.name}_context.{self.config.output_format}'\n            if self.config.output_format == 'jsonl':\n                async with aiofiles.open(output_file, 'w') as f:\n                    metadata = {'timestamp': datetime.now().isoformat(), 'repository_root': str(self.config.target_dir), 'total_files': len(results), 'statistics': self.stats}\n                    await f.write(json.dumps(metadata) + '\\n')\n                    for result in results:\n                        await f.write(json.dumps(result) + '\\n')\n            else:\n                output = {'metadata': {'timestamp': datetime.now().isoformat(), 'repository_root': str(self.config.target_dir), 'statistics': self.stats}, 'files': results}\n                async with aiofiles.open(output_file, 'w') as f:\n                    await f.write(json.dumps(output, indent=2))\n            end_time = asyncio.get_event_loop().time()\n            self.stats['processing_time'] = end_time - start_time\n            if self.stats['failed_files'] > 0:\n                self.logger.error('\\nFailed files details:')\n                for fail_info in self.stats['failed_files_info']:\n                    self.logger.error(f\"\\nFile: {fail_info['file']}\")\n                    self.logger.error(f\"Error: {fail_info['error']}\")\n                    if 'traceback' in fail_info:\n                        self.logger.error(f\"Traceback: {fail_info['traceback']}\")\n            return self.stats\n        except Exception as e:\n            self.logger.error(f'Error during processing: {e}')\n            self.logger.error(traceback.format_exc())\n            raise", "hash": "7205e877802f45624a6519a5f15127e729567130e6ad50e2afca9aa95cb6169f"}
{"path": "processors/constants.py", "type": "py", "analysis": {"success": true, "metrics": {"lines_of_code": 52, "comment_lines": 0, "blank_lines": 5, "complexity": 4, "maintainability_index": 100.0, "max_depth": 0}, "imports": ["dataclasses", "typing"], "functions": [{"name": "file_result", "arguments": ["cls", "path", "file_type", "analysis", "size", "content", "file_hash"], "decorators": ["classmethod"], "is_async": false}, {"name": "metrics_result", "arguments": ["cls", "metrics"], "decorators": ["classmethod"], "is_async": false}, {"name": "function_info", "arguments": ["cls", "name", "args", "decorators", "is_async"], "decorators": ["classmethod"], "is_async": false}, {"name": "class_info", "arguments": ["cls", "name", "methods", "bases"], "decorators": ["classmethod"], "is_async": false}], "classes": [{"name": "AnalysisKeys", "methods": [{"name": "file_result", "arguments": [], "decorators": ["classmethod"], "is_async": false}, {"name": "metrics_result", "arguments": [], "decorators": ["classmethod"], "is_async": false}, {"name": "function_info", "arguments": [], "decorators": ["classmethod"], "is_async": false}, {"name": "class_info", "arguments": [], "decorators": ["classmethod"], "is_async": false}], "base_classes": []}]}, "size": 2274, "content": "from dataclasses import dataclass\nfrom typing import Dict, Any\n\n@dataclass\nclass AnalysisKeys:\n    \"\"\"Keys used in code analysis results\"\"\"\n    PATH: str = 'file_path'\n    TYPE: str = 'file_type'\n    SIZE: str = 'size'\n    CONTENT: str = 'content'\n    HASH: str = 'content_hash'\n    ANALYSIS: str = 'analysis'\n    METRICS: str = 'metrics'\n    IMPORTS: str = 'imports'\n    EXPORTS: str = 'exports'\n    FUNCTIONS: str = 'functions'\n    CLASSES: str = 'classes'\n    SUCCESS: str = 'success'\n    ERROR: str = 'error'\n    LINES_OF_CODE: str = 'lines_of_code'\n    COMMENT_LINES: str = 'comment_lines'\n    BLANK_LINES: str = 'blank_lines'\n    COMPLEXITY: str = 'complexity'\n    MAINTAINABILITY: str = 'maintainability_index'\n    MAX_DEPTH: str = 'max_depth'\n    NAME: str = 'name'\n    ARGS: str = 'arguments'\n    DECORATORS: str = 'decorators'\n    IS_ASYNC: str = 'is_async'\n    IS_PRIVATE: str = 'is_private'\n    BASES: str = 'base_classes'\n    METHODS: str = 'methods'\n\n    @classmethod\n    def file_result(cls, path: str, file_type: str, analysis: Dict[str, Any], size: int, content: str, file_hash: str) -> Dict[str, Any]:\n        \"\"\"Create a standardized file analysis result\"\"\"\n        return {cls.PATH: path, cls.TYPE: file_type, cls.ANALYSIS: analysis, cls.SIZE: size, cls.CONTENT: content, cls.HASH: file_hash}\n\n    @classmethod\n    def metrics_result(cls, metrics: 'CodeMetrics') -> Dict[str, Any]:\n        \"\"\"Create a standardized metrics result\"\"\"\n        return {cls.LINES_OF_CODE: metrics.lines_of_code, cls.COMMENT_LINES: metrics.comment_lines, cls.BLANK_LINES: metrics.blank_lines, cls.COMPLEXITY: metrics.complexity, cls.MAINTAINABILITY: metrics.maintainability_index, cls.MAX_DEPTH: metrics.max_depth}\n\n    @classmethod\n    def function_info(cls, name: str, args: list=None, decorators: list=None, is_async: bool=False) -> Dict[str, Any]:\n        \"\"\"Create a standardized function info dictionary\"\"\"\n        return {cls.NAME: name, cls.ARGS: args or [], cls.DECORATORS: decorators or [], cls.IS_ASYNC: is_async}\n\n    @classmethod\n    def class_info(cls, name: str, methods: list=None, bases: list=None) -> Dict[str, Any]:\n        \"\"\"Create a standardized class info dictionary\"\"\"\n        return {cls.NAME: name, cls.METHODS: methods or [], cls.BASES: bases or []}", "hash": "6977930643edd7af382df69c2c07f3d5cd4625a74245da8a0fecbf0fecbf628c"}
{"path": "processors/code_analyzer.py", "type": "py", "analysis": {"success": true, "metrics": {"lines_of_code": 129, "comment_lines": 0, "blank_lines": 10, "complexity": 25, "maintainability_index": 100.0, "max_depth": 5}, "imports": ["logging", "ast", "re", "typing", "constants", "dataclasses"], "functions": [{"name": "__init__", "arguments": ["self"], "decorators": [], "is_async": false}, {"name": "_calculate_complexity", "arguments": ["self", "node"], "decorators": [], "is_async": false}, {"name": "_calculate_depth", "arguments": ["self", "node", "current"], "decorators": [], "is_async": false}, {"name": "_collect_imports", "arguments": ["self", "tree"], "decorators": [], "is_async": false}, {"name": "analyze_python", "arguments": ["self", "content"], "decorators": [], "is_async": false}, {"name": "analyze_javascript", "arguments": ["self", "content"], "decorators": [], "is_async": false}, {"name": "analyze_code", "arguments": ["self", "content", "file_type"], "decorators": [], "is_async": false}, {"name": "clean_content", "arguments": ["self", "content", "file_type"], "decorators": [], "is_async": false}], "classes": [{"name": "CodeMetrics", "methods": [], "base_classes": []}, {"name": "CodeAnalyzer", "methods": [{"name": "__init__", "arguments": [], "decorators": [], "is_async": false}, {"name": "_calculate_complexity", "arguments": [], "decorators": [], "is_async": false}, {"name": "_calculate_depth", "arguments": [], "decorators": [], "is_async": false}, {"name": "_collect_imports", "arguments": [], "decorators": [], "is_async": false}, {"name": "analyze_python", "arguments": [], "decorators": [], "is_async": false}, {"name": "analyze_javascript", "arguments": [], "decorators": [], "is_async": false}, {"name": "analyze_code", "arguments": [], "decorators": [], "is_async": false}, {"name": "clean_content", "arguments": [], "decorators": [], "is_async": false}], "base_classes": []}]}, "size": 6896, "content": "import ast\nimport re\nfrom dataclasses import dataclass\nfrom typing import Dict, Any, List, Optional, Set\nimport logging\nfrom .constants import AnalysisKeys as Keys\n\n@dataclass\nclass CodeMetrics:\n    lines_of_code: int = 0\n    comment_lines: int = 0\n    blank_lines: int = 0\n    complexity: int = 0\n    maintainability_index: float = 100.0\n    max_depth: int = 0\n\nclass CodeAnalyzer:\n    \"\"\"Analyzes source code files with language-specific handling.\"\"\"\n\n    def __init__(self):\n        self.logger = logging.getLogger(__name__)\n\n    def _calculate_complexity(self, node: ast.AST) -> int:\n        \"\"\"Calculate cyclomatic complexity.\"\"\"\n        complexity = 0\n        for child in ast.walk(node):\n            if isinstance(child, (ast.If, ast.While, ast.For, ast.AsyncFor, ast.ExceptHandler, ast.With, ast.AsyncWith, ast.Assert, ast.Raise)):\n                complexity += 1\n            elif isinstance(child, ast.BoolOp):\n                complexity += len(child.values) - 1\n            elif isinstance(child, ast.IfExp):\n                complexity += 1\n        return max(1, complexity)\n\n    def _calculate_depth(self, node: ast.AST, current: int=0) -> int:\n        \"\"\"Calculate maximum nesting depth.\"\"\"\n        if isinstance(node, (ast.If, ast.For, ast.While, ast.With, ast.AsyncFor, ast.AsyncWith)):\n            current += 1\n        max_depth = current\n        for child in ast.iter_child_nodes(node):\n            child_depth = self._calculate_depth(child, current)\n            max_depth = max(max_depth, child_depth)\n        return max_depth\n\n    def _collect_imports(self, tree: ast.AST) -> Set[str]:\n        \"\"\"Collect all unique imports.\"\"\"\n        imports = set()\n        for node in ast.walk(tree):\n            if isinstance(node, ast.Import):\n                for name in node.names:\n                    imports.add(name.name.split('.')[0])\n            elif isinstance(node, ast.ImportFrom) and node.module:\n                imports.add(node.module.split('.')[0])\n        return imports\n\n    def analyze_python(self, content: str) -> Dict[str, Any]:\n        \"\"\"Analyze Python source code.\"\"\"\n        try:\n            tree = ast.parse(content)\n            lines = content.splitlines()\n            metrics = CodeMetrics(lines_of_code=len(lines), blank_lines=sum((1 for line in lines if not line.strip())), comment_lines=sum((1 for line in lines if line.strip().startswith('#'))))\n            metrics.complexity = self._calculate_complexity(tree)\n            metrics.max_depth = self._calculate_depth(tree)\n            imports = self._collect_imports(tree)\n            functions = []\n            classes = []\n            for node in ast.walk(tree):\n                if isinstance(node, ast.FunctionDef):\n                    functions.append(Keys.function_info(name=node.name, args=[arg.arg for arg in node.args.args], decorators=[ast.unparse(d) for d in node.decorator_list], is_async=isinstance(node, ast.AsyncFunctionDef)))\n                elif isinstance(node, ast.ClassDef):\n                    methods = []\n                    for child in node.body:\n                        if isinstance(child, ast.FunctionDef):\n                            methods.append(Keys.function_info(name=child.name, is_async=isinstance(child, ast.AsyncFunctionDef), decorators=[ast.unparse(d) for d in child.decorator_list]))\n                    classes.append(Keys.class_info(name=node.name, methods=methods, bases=[ast.unparse(base) for base in node.bases]))\n            return {Keys.SUCCESS: True, Keys.METRICS: Keys.metrics_result(metrics), Keys.IMPORTS: list(imports), Keys.FUNCTIONS: functions, Keys.CLASSES: classes}\n        except Exception as e:\n            self.logger.error(f'Error analyzing Python code: {e}')\n            return {Keys.SUCCESS: False, Keys.ERROR: str(e)}\n\n    def analyze_javascript(self, content: str) -> Dict[str, Any]:\n        \"\"\"Analyze JavaScript/TypeScript source code.\"\"\"\n        try:\n            content = re.sub('//.*$', '', content, flags=re.MULTILINE)\n            content = re.sub('/\\\\*[\\\\s\\\\S]*?\\\\*/', '', content)\n            lines = content.splitlines()\n            metrics = CodeMetrics(lines_of_code=len(lines), blank_lines=sum((1 for line in lines if not line.strip())))\n            control_structures = len(re.findall('\\\\b(if|for|while|switch)\\\\b', content))\n            boolean_ops = len(re.findall('\\\\b(&&|\\\\|\\\\|)\\\\b', content))\n            ternary_ops = len(re.findall('\\\\?.*:(?![^{]*})', content))\n            metrics.complexity = control_structures + boolean_ops + ternary_ops\n            block_matches = re.findall('{(?:[^{}]*|{(?:[^{}]*|{[^{}]*})*})*}', content)\n            metrics.max_depth = max((1 + s.count('{') for s in block_matches), default=0)\n            imports = re.findall('import\\\\s+.*?from\\\\s+[\\\\\\'\"]([^\\\\\\'\"]+)[\\\\\\'\"]', content)\n            exports = re.findall('export\\\\s+(?:default\\\\s+)?(?:class|function|const|let|var)\\\\s+(\\\\w+)', content)\n            functions = []\n            for match in re.finditer('(?:async\\\\s+)?function\\\\s+(\\\\w+)\\\\s*\\\\((.*?)\\\\)', content):\n                functions.append(Keys.function_info(name=match.group(1), is_async=bool(re.match('\\\\s*async\\\\s+', match.group(0)))))\n            classes = []\n            for match in re.finditer('class\\\\s+(\\\\w+)(?:\\\\s+extends\\\\s+(\\\\w+))?\\\\s*\\\\{', content):\n                classes.append(Keys.class_info(name=match.group(1), bases=[match.group(2)] if match.group(2) else []))\n            return {Keys.SUCCESS: True, Keys.METRICS: Keys.metrics_result(metrics), Keys.IMPORTS: imports, Keys.EXPORTS: exports, Keys.FUNCTIONS: functions, Keys.CLASSES: classes}\n        except Exception as e:\n            self.logger.error(f'Error analyzing JavaScript code: {e}')\n            return {Keys.SUCCESS: False, Keys.ERROR: str(e)}\n\n    def analyze_code(self, content: str, file_type: str) -> Dict[str, Any]:\n        \"\"\"Analyze code content based on file type.\"\"\"\n        if file_type.lower() in {'py'}:\n            return self.analyze_python(content)\n        elif file_type.lower() in {'js', 'jsx', 'ts', 'tsx'}:\n            return self.analyze_javascript(content)\n        else:\n            return {Keys.SUCCESS: False, Keys.ERROR: f'Unsupported file type: {file_type}'}\n\n    def clean_content(self, content: str, file_type: str) -> str:\n        \"\"\"Remove comments and normalize whitespace.\"\"\"\n        if file_type.lower() == 'py':\n            try:\n                tree = ast.parse(content)\n                return ast.unparse(tree)\n            except:\n                content = re.sub('#.*$', '', content, flags=re.MULTILINE)\n                content = re.sub('\"\"\"[\\\\s\\\\S]*?\"\"\"', '', content)\n                content = re.sub(\"'''[\\\\s\\\\S]*?'''\", '', content)\n        else:\n            content = re.sub('//.*$', '', content, flags=re.MULTILINE)\n            content = re.sub('/\\\\*[\\\\s\\\\S]*?\\\\*/', '', content)\n        return '\\n'.join((line for line in content.splitlines() if line.strip()))", "hash": "c2014f6ac958fab99bc50765c8aac7e11d976928e77dffdff2c527508f35f136"}
